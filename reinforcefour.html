<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BBZ8BXY4N7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BBZ8BXY4N7');
</script>
  <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>강화학습 기반 마이크로그리드 control - 4) DDPG를 이용한 ‘continuous’ control 도출 | 에너지엔데이터연구소</title>
<meta name="description" content="실제로는 수전/송전이 continuous action임에도, Vincent의 마이크로그리드 사례에선 ‘discrete’ action (1.1kW 수전/ 1.1kW 송전/ idle)으로도 충분히 economic control이 가능했다. 그렇다면, continuous action을 다루는 심층강화학습 기법을 적용하면 더 우수한 economic control이 가능할 지 궁금해진다.">


  <meta name="author" content="Jeonghun Song">
  
  <meta property="article:author" content="Jeonghun Song">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="에너지엔데이터연구소">
<meta property="og:title" content="강화학습 기반 마이크로그리드 control - 4) DDPG를 이용한 ‘continuous’ control 도출">
<meta property="og:url" content="https://song4energyndata.github.io/reinforcefour.html">


  <meta property="og:description" content="실제로는 수전/송전이 continuous action임에도, Vincent의 마이크로그리드 사례에선 ‘discrete’ action (1.1kW 수전/ 1.1kW 송전/ idle)으로도 충분히 economic control이 가능했다. 그렇다면, continuous action을 다루는 심층강화학습 기법을 적용하면 더 우수한 economic control이 가능할 지 궁금해진다.">



  <meta property="og:image" content="https://song4energyndata.github.io/assets/images/reinforcefour/result_ddpg.png">





  <meta property="article:published_time" content="2023-06-25T00:00:00+09:00">





  

  


<link rel="canonical" href="https://song4energyndata.github.io/reinforcefour.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "송정훈",
      "url": "https://song4energyndata.github.io/"
    
  }
</script>






  <meta name="naver-site-verification" content="7dda6777a2acc897ba51668ac7c58c380307dd6c">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="에너지엔데이터연구소 Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<link rel="alternate" type="application/rss+xml" href="https://song4energyndata.github.io/feed.xml">
  <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<head>
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
</head>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- end custom head snippets -->
  <style> 
    ::-webkit-scrollbar{width: 12px;}
    ::-webkit-scrollbar-track {background-color:#ced8e7; border-radius: 8px;}
    ::-webkit-scrollbar-thumb {background-color:#eeebeb; border-radius: 8px;}
    ::-webkit-scrollbar-thumb:hover {background: #f7f6f3;}
    ::-webkit-scrollbar-button:start:decrement,::-webkit-scrollbar-button:end:increment 
    {width:12px;height:16px;background:transparent;} 
  </style>
</head>

<body
  class="layout--single">
  <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

  

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          에너지엔데이터연구소
          <span class="site-subtitle">에너지 데이터의 올바른 활용, 에너지 전환에 대한 데이터 기반의 합리적 솔루션</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/aboutme/">About Me</a>
            </li><li class="masthead__menu-item">
              <a href="/search/">Search</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tag</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">카테고리 목록</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


  <div class="initial-content">
    





<div id="main" role="main">
  

  <div class="sidebar sticky">
    
  
  
  
  
    
    
      
    
    
  
  
   <!-- three lines added by Jeonghun  -->
    

<nav class="nav__list">
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">카테고리 목록</label>
  <ul class="nav__items" id="category_tag_menu">
      <!--전체 글 수-->
      
      <li>
        <!--span 태그로 카테고리들을 크게 분류 ex) C/C++/C#-->
        <span class="nav__sub-title">Categories</span>
            <!--ul 태그로 같은 카테고리들 모아둔 페이지들 나열-->
            <ul>
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/optimalsystem" class="">&#x2022 경제성분석 및 최적구성 도출 [11]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/market" class="">&#x2022 전력시장 모델링 [15]</a></li>
                    
                
                    
                
            </ul>
            <ul>
                <!--Cpp 카테고리 글들을 모아둔 페이지인 /categories/cpp 주소의 글로 링크 연결-->
                <!--category[1].size 로 해당 카테고리를 가진 글의 개수 표시--> 
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/energymanagement" class="">&#x2022 제어 (최적화/ 강화학습) [5]</a></li>
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/estimation" class="">&#x2022 예측 (회귀분석/ 머신러닝/ 딥러닝) [2]</a></li>
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/dataset" class="">&#x2022 데이터셋 (소개/ 전처리/ 활용안) [7]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/policy" class="">&#x2022 정책 (패널분석/ 시나리오분석) [1]</a></li>
                    
                
            </ul>
            <ul>
                
                    
                
                    
                        <li><a href="/mathstat" class="">&#x2022 수학/ 통계학 일반 [3]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                        <li><a href="/mypapers" class="">&#x2022 My papers [2]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                        <li><a href="/etc" class="">&#x2022 Etc. [2]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
      </li>
  </ul>
</nav>
  
  

  </div>




  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="강화학습 기반 마이크로그리드 control - 4) DDPG를 이용한 ‘continuous’ control 도출">
    <meta itemprop="description" content="실제로는 수전/송전이 continuous action임에도, Vincent의 마이크로그리드 사례에선 ‘discrete’ action (1.1kW 수전/ 1.1kW 송전/ idle)으로도 충분히 economic control이 가능했다. 그렇다면, continuous action을 다루는 심층강화학습 기법을 적용하면 더 우수한 economic control이 가능할 지 궁금해진다.">
    <meta itemprop="datePublished" content="2023-06-25T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="https://song4energyndata.github.io/reinforcefour.html" class="u-url" itemprop="url">강화학습 기반 마이크로그리드 control - 4) DDPG를 이용한 ‘continuous’ control 도출
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-06-25T00:00:00+09:00">2023-06-25</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header> 
              <ul class="toc__menu"><li><a href="#actor와-critic-신경망">Actor와 critic 신경망</a></li><li><a href="#target-신경망">Target 신경망</a></li><li><a href="#ddpg-코드">DDPG 코드</a></li><li><a href="#ddpg를-통한-continuous-control-결과">DDPG를 통한 continuous control 결과</a></li></ul>
 <!-- 우측 TOC -->
            </nav>
          </aside>
        
        <p>실제로는 수전/송전이 continuous action임에도, Vincent의 마이크로그리드 사례에선 ‘discrete’ action (1.1kW 수전/ 1.1kW 송전/ idle)으로도 충분히 economic control이 가능했다. 그렇다면, continuous action을 다루는 심층강화학습 기법을 적용하면 더 우수한 economic control이 가능할 지 궁금해진다.</p>

<p><img src="/assets/images/reinforceone/system.png" alt="system" class="align-center" />
<em>Vincent의 연구에서 가정된 마이크로그리드.</em></p>

<p>이번에는 continuous control의 기본적인 방법인 <a href="https://arxiv.org/abs/1509.02971">Deep Deterministic Policy Gradient (DDPG)</a>를 적용해서, 수전/송전에 대한 continuous control을 도출해본다.</p>

<p><br /></p>

<h2 id="actor와-critic-신경망">Actor와 critic 신경망</h2>
<p>Continuous action인 경우 가능한 action의 수가 사실상 무한대이므로, DQN에서처럼 state를 입력받아 각 action 별 Q-value를 모두 계산하는 신경망을 구성할 수는 없다.</p>

<p>대신에, 두 개의 신경망을 구성하되 하나는 state를 입력받고 continuous action의 값을 출력하고, 다른 하나는 state와 action을 입력받아 그 state-action pair의 Q-value를 출력하도록 한다. 두 신경망 중 전자를 actor (action을 결정함), 후자를 critic (value를 평가함) 이라 부른다.</p>

<p>이 때 actor는 state를 받고 deterministic한 action을 출력하는 policy로 볼 수 있다. 그래서 이 방법에는 Deep Deterministic Policy Gradient (DDPG) 라는 이름이 붙었다.</p>

<p>특히 이 마이크로그리드 사례에서 action은 하한이 -1.1, 상한이 1.1로 정해져 있다. 이러한 ‘bounded’ action을 구현하기 위해, actor의 output layer에 tanh (하이퍼볼릭 탄젠트) activation function을 걸어준다.</p>

<p><br /></p>

<h2 id="target-신경망">Target 신경망</h2>
<p>한편, DDPG에서는 ‘target’ 신경망을 구성해서 학습의 안정성을 향상시킨다. Actor와 critic 각각의 target 신경망들은 원래 actor와 critic의 복사본으로 시작해서, Q-learning에서 next state에 대한 값을 도출하는 데 이용된 후 천천히 업데이트된다.</p>

<p>구체적으로, 원래 신경망들의 parameter를 $\theta$, target 신경망들의 parameter를 $\theta’$라 하고, 원래 actor와 target actor를 각각 $\mu(s)$와 $\mu’(s)$, 원래 critic과 target critic을 각각 $Q(s,a)$와 $Q’(s,a)$라 하자.</p>

<p>그러면, critic 신경망 훈련 시의 목적함수는 $Q(s_{t},a_{t})$와 $r_{t+1} + \gamma \text{max}_{\mu’} Q’(s_{t+1},\mu’(s_{t+1}))$ 간 차이의 평균제곱합이다. 두 번째 항이 target 신경망에 의해 계산되었음에 주목한다.</p>

<p>한편 actor 신경망 훈련 시의 목적함수는 $-Q(s_{t},\mu(s_{t}))$, 즉 음의 Q-value이다. Actor는 Q-value를 최대화하는 action을 도출해야 하기 때문이다. Actor 훈련 시의 목적함수는 target 신경망에 의해 계산되지 않았다.</p>

<p>원래 critic과 actor에 대해 가중치 업데이트를 수행한 후, target critic과 actor에 대한 가중치 업데이트는 작은 양수 $\tau$에 대해 아래와 같이 수행한다. $\tau$가 작으므로 (대략 0.05 전후), $\theta’$는 ‘느리게’ 업데이트된다.</p>

<p>$\theta’ \leftarrow \tau \theta + (1-\tau) \theta’$</p>

<p>왜 target 신경망을 쓰면 학습의 안정성이 향상되는 것일까?</p>

<p>Q-learning에서는 $q_{\ast}(s_{t},a_{t})$에 대한 두 추정치 $Q(s_{t},a_{t})$와 $r_{t+1} + \gamma \text{max}_{a_{t+1}} Q(s_{t+1},a_{t+1})$가 말 그대로 ‘둘 다 추정치’이다.</p>

<p>즉 추정치를 추정치 기반으로 추정한다 (이를 bootstrap이라고도 한다). 이 때문에, 운 나쁘게 추정이 잘못된 방향으로 진행되기 시작하면, 훈련이 발산해버리기 쉽다.</p>

<p>Target 신경망 구성을 통해, 두 추정치 중 후자의 변화를 완화하면, 훈련이 발산할 가능성을 크게 줄일 수 있다.</p>

<p>(보다 더 상세한 내용은 <a href="https://arxiv.org/abs/1509.02971">논문 원문</a>을 참고하길 바란다)</p>

<p><br /></p>

<h2 id="ddpg-코드">DDPG 코드</h2>
<p>이제 DDPG를 적용하여 continuous controller를 훈련하는 코드를 보자 <a href="https://antonai.blog/reinforcement-learning-in-continuous-action-spaces-part-1-ddpg/">(해당 코드 작성에 참고한 DDPG 코딩 설명 포스트).</a> 코드 내 주석은 지난 포스팅의 DQN 코드 대비 다른 부분에 대해서만 추가하였다. (<a href="https://github.com/song4energyndata/Codes/tree/main/reinforcementlearning/microgrid_Vincent">GitHub Repo 링크</a>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -*- coding: utf-8 -*-
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">concatenate</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="c1">### hyperparameters
</span>
<span class="n">actor_lr</span> <span class="o">=</span> <span class="mf">0.0005</span> <span class="c1"># actor NN의 learning rate
</span><span class="n">critic_lr</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="c1"># critic NN의 learning rate
</span><span class="n">tau</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># target NN 업데이트 속도 조절 계수
</span><span class="n">rewardscalefactor</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.98</span>
<span class="n">period_step_fortrain</span> <span class="o">=</span> <span class="mi">24</span>



<span class="c1">### microgrid system data
</span>
<span class="n">PV_prod_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'BelgiumPV_prod_train.npy'</span><span class="p">)</span> 
<span class="n">PV_prod_test</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'BelgiumPV_prod_test.npy'</span><span class="p">)</span> 

<span class="n">load_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'example_nondeterminist_cons_train.npy'</span><span class="p">)</span>
<span class="n">load_test</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'example_nondeterminist_cons_test.npy'</span><span class="p">)</span>

<span class="n">prate_h2</span> <span class="o">=</span> <span class="mf">1.1</span>
<span class="n">eff_h2</span> <span class="o">=</span> <span class="mf">0.65</span>

<span class="n">capa_batt</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">eff_batt</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">initialenergy_batt</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="n">price_h2</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">cost_loss</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">load_peak</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pv_peak</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">inputlen_load</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">inputlen_pv</span> <span class="o">=</span> <span class="mi">24</span>



<span class="c1">### Neural net configuration
</span>
<span class="k">class</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span> <span class="c1"># Q-value를 추정하는 NN (action이 continuous)
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Critic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span> <span class="c1"># tf.Keras.Model 상속받는 이유는 main코드에서 get_weights, set_weights 등을 매번 model 메서드를 불러오지 않고도 편하게 쓰기 위함          
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_load</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">input_pv</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_load</span> <span class="o">=</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">input_load</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_pv</span> <span class="o">=</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">input_pv</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">concat_conv</span> <span class="o">=</span> <span class="nf">concatenate</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_load</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_pv</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_concat</span> <span class="o">=</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">concat_conv</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">flatten_conv</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">()(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_concat</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_others</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_action</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># action 값 (state 뿐 아니라 action도 input임)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">concat_all</span> <span class="o">=</span> <span class="nf">concatenate</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">flatten_conv</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_others</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_action</span><span class="p">])</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_1</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">concat_all</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_2</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_qval</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">input_load</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_pv</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_others</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_action</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">output_qval</span><span class="p">])</span>     
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">,</span><span class="n">input_action</span><span class="p">):</span> <span class="c1"># state 뿐 아니라 action도 input임        
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">((</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">,</span><span class="n">input_action</span><span class="p">))</span> <span class="c1"># output은 입력된 state-action pair에 대한 Q-value '단일값'
</span>
<span class="k">class</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span> <span class="c1"># (Continuous) Action을 결정하는 policy NN
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Actor</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>        
        <span class="n">self</span><span class="p">.</span><span class="n">input_load</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">input_pv</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_load</span> <span class="o">=</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">input_load</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_pv</span> <span class="o">=</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">input_pv</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">concat_conv</span> <span class="o">=</span> <span class="nf">concatenate</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_load</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_pv</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_concat</span> <span class="o">=</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">concat_conv</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">flatten_conv</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">()(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_conv_concat</span><span class="p">)</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">input_others</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">concat_all</span> <span class="o">=</span> <span class="nf">concatenate</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">flatten_conv</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_others</span><span class="p">])</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_1</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">concat_all</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_2</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_action</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'tanh'</span><span class="p">)(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_dense_2</span><span class="p">)</span> <span class="c1"># Node 갯수는 action의 자유도이며, tanh activation은 action(충/방전) 이 [-1,1] 범위의 bounded action임을 반영함
</span>        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">input_load</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_pv</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_others</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">output_action</span><span class="p">])</span>     
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">):</span> <span class="c1"># state가 input임        
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">((</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">))</span>

<span class="n">critic_one_learning</span> <span class="o">=</span> <span class="nc">Critic</span><span class="p">()</span> 
<span class="n">critic_one_target</span> <span class="o">=</span> <span class="nc">Critic</span><span class="p">()</span> <span class="c1"># target 신경망 정의
</span><span class="n">critic_one_learning</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">critic_lr</span><span class="p">))</span>
<span class="n">critic_one_target</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">critic_lr</span><span class="p">))</span>
<span class="n">critic_one_target</span><span class="p">.</span><span class="nf">set_weights</span><span class="p">(</span><span class="n">critic_one_learning</span><span class="p">.</span><span class="nf">get_weights</span><span class="p">())</span> <span class="c1"># target 신경망의 가중치의 초기값은 원래 신경망의 가중치와 같게 둠.
</span>
<span class="n">actor_learning</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">()</span>
<span class="n">actor_target</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">()</span> <span class="c1"># target 신경망 정의
</span><span class="n">actor_learning</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">actor_lr</span><span class="p">))</span>
<span class="n">actor_target</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">actor_lr</span><span class="p">))</span>
<span class="n">actor_target</span><span class="p">.</span><span class="nf">set_weights</span><span class="p">(</span><span class="n">actor_learning</span><span class="p">.</span><span class="nf">get_weights</span><span class="p">())</span> <span class="c1"># target 신경망의 가중치의 초기값은 원래 신경망의 가중치와 같게 둠.
</span>


<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample_experiences</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span> 
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span> 
    <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">replay_buffer</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span> 
    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">experience</span><span class="p">[</span><span class="n">field_index</span><span class="p">]</span> <span class="k">for</span> <span class="n">experience</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span> 
        <span class="k">for</span> <span class="n">field_index</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span>



<span class="k">def</span> <span class="nf">play_one_step</span><span class="p">(</span><span class="n">profile_load</span><span class="p">,</span><span class="n">profile_pv</span><span class="p">,</span><span class="n">hour</span><span class="p">,</span><span class="n">energy_batt</span><span class="p">,</span><span class="n">epsilon</span><span class="p">,</span><span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">(</span><span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="n">inputlen_load</span><span class="p">:</span><span class="n">hour</span><span class="p">],</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="n">inputlen_pv</span><span class="p">:</span><span class="n">hour</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">energy_batt</span><span class="p">]))</span> <span class="p">)</span>
    
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span> <span class="ow">and</span> <span class="n">training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># exploration during training
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># exploration 시 -1~+1 사이의 실수를 균등분포에서 추출
</span>    <span class="k">else</span><span class="p">:</span> <span class="c1"># exploitation
</span>        <span class="n">action</span> <span class="o">=</span> <span class="nf">actor_learning</span><span class="p">(</span><span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="n">inputlen_load</span><span class="p">:</span><span class="n">hour</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="n">inputlen_pv</span><span class="p">:</span><span class="n">hour</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">energy_batt</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)).</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># action net의 output은 tensor이므로, 이를 numpy 변환 후 [0][0]으로 불러와야 int 변수가 됨
</span>    
    <span class="c1"># Unscaling
</span>    <span class="n">p_h2_send</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_h2_receive</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">p_h2_receive</span> <span class="o">=</span> <span class="n">prate_h2</span><span class="o">*</span><span class="n">action</span> <span class="c1"># 여기서 action은 전기 '수전'에 대해 양수라고 가정, action은 [-1,1] 구간의 실수이므로 prate_h2를 곱해 [-1.1,1.1] 구간의 실수가 되도록 함
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">p_h2_send</span> <span class="o">=</span> <span class="o">-</span><span class="n">prate_h2</span><span class="o">*</span><span class="n">action</span> <span class="c1"># action은 전기 '송전'에 대해 음수
</span>    <span class="n">p_load</span> <span class="o">=</span> <span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="p">]</span><span class="o">*</span><span class="n">load_peak</span> 
    <span class="n">p_pv</span> <span class="o">=</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="p">]</span><span class="o">*</span><span class="n">pv_peak</span> 
    <span class="n">energy_batt</span> <span class="o">=</span> <span class="n">energy_batt</span><span class="o">*</span><span class="n">capa_batt</span> 
        
    <span class="c1">#p_curtail = 0
</span>    <span class="n">p_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">p_net_beforebatt</span> <span class="o">=</span> <span class="n">p_pv</span> <span class="o">-</span> <span class="n">p_load</span> <span class="o">+</span> <span class="n">p_h2_receive</span> <span class="o">-</span> <span class="n">p_h2_send</span> 
    
    <span class="k">if</span> <span class="n">p_net_beforebatt</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">capa_batt</span> <span class="o">&gt;=</span> <span class="n">energy_batt</span> <span class="o">+</span> <span class="n">p_net_beforebatt</span><span class="o">*</span><span class="n">eff_batt</span><span class="p">:</span> 
            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">energy_batt</span> <span class="o">+</span> <span class="n">p_net_beforebatt</span><span class="o">*</span><span class="n">eff_batt</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">capa_batt</span>
            <span class="c1"># p_curtail = (energy_batt + p_net_beforebatt*eff_batt - capa_batt)/eff_batt 
</span>    <span class="k">else</span><span class="p">:</span> 
        <span class="k">if</span> <span class="n">energy_batt</span><span class="o">*</span><span class="n">eff_batt</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">p_net_beforebatt</span><span class="p">:</span> 
            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">energy_batt</span> <span class="o">+</span> <span class="n">p_net_beforebatt</span><span class="o">/</span><span class="n">eff_batt</span> 
        <span class="k">else</span><span class="p">:</span> 
            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">p_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_net_beforebatt</span> <span class="o">-</span> <span class="n">energy_batt</span><span class="o">*</span><span class="n">eff_batt</span>           
    
    <span class="n">reward</span> <span class="o">=</span> <span class="n">price_h2</span><span class="o">*</span><span class="n">p_h2_send</span><span class="o">*</span><span class="n">eff_h2</span> <span class="o">-</span> <span class="n">price_h2</span><span class="o">*</span><span class="n">p_h2_receive</span><span class="o">/</span><span class="n">eff_h2</span> <span class="o">-</span> <span class="n">cost_loss</span><span class="o">*</span><span class="n">p_loss</span> 
    <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">energy_batt_after</span><span class="o">/</span><span class="n">capa_batt</span> 
    
    <span class="k">if</span> <span class="n">training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">(</span><span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="p">(</span><span class="n">inputlen_load</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span><span class="n">hour</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="p">(</span><span class="n">inputlen_pv</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span><span class="n">hour</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">energy_batt_after</span><span class="p">]))</span> <span class="p">)</span>         
        <span class="n">replay_buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="o">*</span><span class="n">rewardscalefactor</span><span class="p">,</span> <span class="n">next_state</span><span class="p">))</span> 
    <span class="k">return</span> <span class="n">energy_batt_after</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">action</span>



<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">actorupdate</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    
    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="nf">sample_experiences</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        
    <span class="n">input_load</span> <span class="o">=</span> <span class="n">states</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">inputlen_load</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> 
    <span class="n">input_pv</span> <span class="o">=</span> <span class="n">states</span><span class="p">[:,</span><span class="n">inputlen_load</span><span class="p">:(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_others</span> <span class="o">=</span> <span class="n">states</span><span class="p">[:,(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">input_load_next</span> <span class="o">=</span> <span class="n">next_states</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">inputlen_load</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_pv_next</span> <span class="o">=</span> <span class="n">next_states</span><span class="p">[:,</span><span class="n">inputlen_load</span><span class="p">:(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_others_next</span> <span class="o">=</span> <span class="n">next_states</span><span class="p">[:,(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="n">actions_by_target</span> <span class="o">=</span> <span class="nf">actor_target</span><span class="p">(</span><span class="n">input_load_next</span><span class="p">,</span><span class="n">input_pv_next</span><span class="p">,</span><span class="n">input_others_next</span><span class="p">).</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># nextstate에 대한 action은 target net으로 도출
</span>    <span class="n">actions_by_target</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">actions_by_target</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># [-1,1] 범위로 제한
</span>    
    <span class="n">Q_values_one_by_target</span> <span class="o">=</span> <span class="nf">critic_one_target</span><span class="p">(</span><span class="n">input_load_next</span><span class="p">,</span><span class="n">input_pv_next</span><span class="p">,</span><span class="n">input_others_next</span><span class="p">,</span><span class="n">actions_by_target</span><span class="p">)</span> <span class="c1"># nextstate에 대한 Q-value는 target net으로 도출
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">stop_gradient</span><span class="p">(</span><span class="n">rewards</span> <span class="o">+</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">Q_values_one_by_target</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span> <span class="c1"># critic net 업데이트를 위한 자동미분 (actor net 업데이트와 별개로 둠)        
</span>        <span class="n">Q_values_one</span> <span class="o">=</span> <span class="nf">critic_one_learning</span><span class="p">(</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">,</span><span class="n">actions</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># critic_one_learning을 gradient descent로 훈련시키려면, GradientTape 구문 내에서 현재 state-action에 대한 Q_value Tensor를 critic_one_learning으로 다시 불러와야 함
</span>        <span class="n">loss_critic_one</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Q_values_one</span><span class="p">))</span> <span class="c1"># 평균제곱오차 계산
</span>    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss_critic_one</span><span class="p">,</span> <span class="n">critic_one_learning</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span> 
    <span class="n">critic_one_learning</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">critic_one_learning</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
     
    <span class="k">if</span> <span class="n">actorupdate</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>    
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span> <span class="c1"># actor net 업데이트를 위한 자동미분 (critic net 업데이트와 별개로 둠)  
</span>            <span class="n">actions_by_learner</span> <span class="o">=</span> <span class="nf">actor_learning</span><span class="p">(</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">)</span> <span class="c1"># actor_learning을 gradient descent로 훈련시키려면, GradientTape 구문 내에서 현재 state에 대한 action Tensor를 actor_learning으로 다시 불러와야 함
</span>            <span class="n">Q_values_one</span> <span class="o">=</span> <span class="nf">critic_one_learning</span><span class="p">(</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">,</span><span class="n">actions_by_learner</span><span class="p">)</span> <span class="c1"># actor_learning으로 다시 불러온 action 기반으로 Q-value 계산
</span>            <span class="n">loss_actor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">Q_values_one</span><span class="p">)</span> <span class="c1"># 음의 Q-value 최소화, 즉 Q-value를 최대화하도록 actor를 업데이트함
</span>        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss_actor</span><span class="p">,</span> <span class="n">actor_learning</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">actor_learning</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">actor_learning</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        
        <span class="n">actor_weights</span> <span class="o">=</span> <span class="n">actor_learning</span><span class="p">.</span><span class="n">weights</span> <span class="c1"># 원래 net의 weight 불러옴 (연산을 위해)
</span>        <span class="n">target_actor_weights</span> <span class="o">=</span> <span class="n">actor_target</span><span class="p">.</span><span class="n">weights</span> <span class="c1"># target net의 weight 불러옴
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">actor_weights</span><span class="p">)):</span> <span class="c1"># Target net 가중치 업데이트
</span>            <span class="n">target_actor_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">actor_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_actor_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># tau가 작으므로 target net이 '천천히' 업데이트됨
</span>        <span class="n">actor_target</span><span class="p">.</span><span class="nf">set_weights</span><span class="p">(</span><span class="n">target_actor_weights</span><span class="p">)</span> <span class="c1"># 업데이트된 가중치로 설정
</span>        
        <span class="n">critic_one_weights</span> <span class="o">=</span> <span class="n">critic_one_learning</span><span class="p">.</span><span class="n">weights</span> <span class="c1"># 원래 net의 weight 불러옴 (연산을 위해)        
</span>        <span class="n">target_critic_one_weights</span> <span class="o">=</span> <span class="n">critic_one_target</span><span class="p">.</span><span class="n">weights</span> <span class="c1"># target net의 weight 불러옴
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">critic_one_weights</span><span class="p">)):</span>  <span class="c1"># Target net 가중치 업데이트
</span>            <span class="n">target_critic_one_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tau</span> <span class="o">*</span> <span class="n">critic_one_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_critic_one_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># tau가 작으므로 target net이 '천천히' 업데이트됨          
</span>        <span class="n">critic_one_target</span><span class="p">.</span><span class="nf">set_weights</span><span class="p">(</span><span class="n">target_critic_one_weights</span><span class="p">)</span> <span class="c1"># 업데이트된 가중치로 설정  
</span>


<span class="n">profits_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">elapsedtime_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">max_return_test</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
<span class="n">count_step_fortrain</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">bool_fortrain</span> <span class="o">=</span> <span class="bp">False</span>


<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span> 
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> 
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    
    <span class="c1">### Train for each epoch
</span>    
    <span class="n">hour</span> <span class="o">=</span> <span class="mi">24</span> 
    <span class="n">energy_batt</span> <span class="o">=</span> <span class="n">initialenergy_batt</span>  
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">load_train</span><span class="p">)</span><span class="o">-</span><span class="mi">25</span><span class="p">):</span> 
        <span class="n">count_step_fortrain</span> <span class="o">+=</span> <span class="mi">1</span>  
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epoch</span> <span class="o">/</span> <span class="mi">50</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span> 
        <span class="n">energy_batt</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="nf">play_one_step</span><span class="p">(</span><span class="n">load_train</span><span class="p">,</span><span class="n">PV_prod_train</span><span class="p">,</span><span class="n">hour</span><span class="p">,</span><span class="n">energy_batt</span><span class="p">,</span><span class="n">epsilon</span><span class="p">,</span><span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">hour</span> <span class="o">+=</span> <span class="mi">1</span>     

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">count_step_fortrain</span> <span class="o">&gt;</span> <span class="n">period_step_fortrain</span><span class="p">:</span> 
                <span class="nf">training_step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">actorupdate</span><span class="o">=</span><span class="n">bool_fortrain</span><span class="p">)</span>
                <span class="n">count_step_fortrain</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">bool_fortrain</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">bool_fortrain</span> <span class="c1"># 한 번은 critic만, 한 번은 actor &amp; target까지 전부 업데이트하는 과정을 교대로 진행
</span>                
                
    <span class="c1">### Validation for each epoch  
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">testcase_actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">testcase_battenergy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">epoch_return_test</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="n">hour</span> <span class="o">=</span> <span class="mi">24</span> <span class="c1"># 시작시점   
</span>        <span class="n">energy_batt</span> <span class="o">=</span> <span class="n">initialenergy_batt</span>
        
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">load_test</span><span class="p">)</span><span class="o">-</span><span class="mi">24</span><span class="p">):</span> 
            <span class="n">energy_batt</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="nf">play_one_step</span><span class="p">(</span><span class="n">load_test</span><span class="p">,</span><span class="n">PV_prod_test</span><span class="p">,</span><span class="n">hour</span><span class="p">,</span><span class="n">energy_batt</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 
            <span class="n">testcase_actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">testcase_battenergy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">energy_batt</span><span class="p">)</span>
            <span class="n">epoch_return_test</span> <span class="o">+=</span> <span class="n">reward</span> 
            <span class="n">hour</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="n">profits_test</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">epoch_return_test</span><span class="p">)</span>    
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_profit_test_ddpg.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">profits_test</span><span class="p">:</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    
        <span class="k">if</span> <span class="n">max_return_test</span> <span class="o">&lt;</span> <span class="n">epoch_return_test</span><span class="p">:</span>
            <span class="n">max_return_test</span> <span class="o">=</span> <span class="n">epoch_return_test</span>
            <span class="n">actor_learning</span><span class="p">.</span><span class="nf">save_weights</span><span class="p">(</span><span class="s">'actor_trainedmodel_ddpg.h5'</span><span class="p">)</span>
            <span class="n">critic_one_learning</span><span class="p">.</span><span class="nf">save_weights</span><span class="p">(</span><span class="s">'critic_one_trainedmodel_ddpg.h5'</span><span class="p">)</span>
                    
            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_actions_test_ddpg.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">testcase_actions</span><span class="p">:</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
            
            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_battenergy_test_ddpg.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">testcase_battenergy</span><span class="p">:</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                        
        <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>   
        <span class="n">elapsedtime_test</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">"Validation: profit of epoch {} is {}, maximum profit is {}"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">epoch_return_test</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="nf">round</span><span class="p">(</span><span class="n">max_return_test</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'one epoch 수행에 {}초 걸렸습니다'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">round</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_time_test_ddpg.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">elapsedtime_test</span><span class="p">:</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p><br /></p>

<h2 id="ddpg를-통한-continuous-control-결과">DDPG를 통한 continuous control 결과</h2>
<p>DDPG 훈련 결과, validation case에서의 누적 비용은 125유로이다.</p>

<p>놀랍게도(?), continuous control (DDPG) 의 결과가 3-action discrete control (DQN) 의 결과 (누적비용 50유로) 보다 나쁘다. 어떻게 된 걸까?</p>

<p>아래 그림으로 action을 비교해 보자.</p>

<p><img src="/assets/images/reinforcefour/result_ddpg.png" alt="result_ddpg" class="align-center" />
<em>DDPG로 도출한 수전/송전 control. LP control과는 차이가 있음.</em></p>

<p>태양광 발전량이 많아 낮에 송전하는 기간의 밤 시간대에 LP와 DQN에선 action의 값이 0, 즉 idle이다. 그러나 DDPG에서는 수전을 한다. 반대로 태양광 발전량이 적어 낮에 수전하는 기간의 밤 시간대에도 LP와 DQN에선 action이 idle인데 DDPG에서는 일부 송전을 한다.</p>

<p>즉 LP와 DQN control에서는 수 시간 연속으로 idle인 기간에, DDPG control에서는 송전 혹은 수전을 행한다. 그런데 이것이 sub-optimal control인 것으로 보인다.</p>

<p>이는 state를 input으로 하는 nonlinear continuous function 근사로는, 대부분의 input에 대해 output이 0, -1, +1 셋 중 하나이고 그 외 일부 input에 대해서만 값이 -1~0 또는 0~1 사이의 값을 출력하는 function을 (LP control에서처럼) 만들어내기 어렵기 때문인 것으로 추측된다.</p>

<p>즉, 미래를 모두 안다고 가정했을 때의 LP control은 DDPG로 도출한 control과는 거리가 있으며 DQN으로 도출한 control에 더 가까운 형태를 띤다. 그렇기 때문에, 놀랍게도(?) Vincent의 마이크로그리드 사례에서는 DQN의 discrete control이 DDPG의 continuous control 대비 더 나은 것으로 보인다.</p>

<p><br />
그러나 누군가는 이렇게 반박할 수 있다. “DDPG는 초창기 기법이다. 몇 년 뒤 Twin Delayed Deep Deterministic policy gradient (TD3)나 Soft Actor-Critic (SAC)처럼, 더 진보된 continuous control 방법들이 나왔다. 더 진보된 방법을 쓰면, 완벽한 continuous control을 얻을 것이다.”</p>

<p>과연 어떨까? 다음 포스팅에서 확인해 보겠다.</p>

<div class="notice--info">

강화학습 기반 마이크로그리드 control<br /><br />

1) <a href="/reinforceone.html">미래를 모를 때의 '경제적' control을 위한 강화학습</a><br />
2) <a href="/reinforcetwo.html">강화학습의 기본, Q-learning 리뷰</a><br />
3) <a href="/reinforcethree.html">Deep Q-Network를 통한 3-action control 도출</a><br />
4) <b>DDPG를 이용한 'continuous' control 도출</b><br />
5) <a href="/reinforcefive.html">TD3/ SAC 등 '진보된' continuous control을 쓴다면?</a>

</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#python" class="page__taxonomy-item p-category" rel="tag">Python</a><span class="sep">, </span>
    
      <a href="/tags/#%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5" class="page__taxonomy-item p-category" rel="tag">강화학습</a><span class="sep">, </span>
    
      <a href="/tags/#%EB%85%B9%EC%83%89%EC%84%AC" class="page__taxonomy-item p-category" rel="tag">녹색섬</a>
    
    </span>
  </p>



<!-- 
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#energymanagement" class="page__taxonomy-item p-category" rel="tag">energymanagement</a>
    
    </span>
  </p>

 -->
        <!-- 

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2023-06-25T00:00:00+09:00">2023-06-25</time></p>
 -->
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <!-- <a href="https://twitter.com/intent/tweet?text=%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5+%EA%B8%B0%EB%B0%98+%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EA%B7%B8%EB%A6%AC%EB%93%9C+control+-+4%29+DDPG%EB%A5%BC+%EC%9D%B4%EC%9A%A9%ED%95%9C+%27continuous%27+control+%EB%8F%84%EC%B6%9C%20http%3A%2F%2Flocalhost%3A4000%2Freinforcefour.html" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> -->

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Freinforcefour.html" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Freinforcefour.html" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      <p></p>
        <!-- <h4 class="page__meta-title"><span>energymanagement</span> <span>카테고리 내 다른 글 보러가기</span></h4> -->
        


  

  
  	
  	
  	
  	
  	

<nav class="pagination_prev_next"> <!-- 식빵맘 코드에서 조금 수정함 -->

  
    
      <a href="/reinforcefive.html" class="pagination_prev_next--pager"><span class="prev_next">다음 글  &nbsp  </span>강화학습 기반 마이크로그리드 control - 5) TD3/ SAC 등 '진보된' continuous control을 쓴다면?</a>
    
    
      <a href="/reinforcethree.html" class="pagination_prev_next--pager"><span class="prev_next">이전 글  &nbsp</span>강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 3-action control 도출</a>
        
  

</nav>
      <p></p>
    </div>

    
  </article>

  
  <!-- 
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <a href="/hourlysolar.html" rel="permalink"><img src="/assets/images/solarradiation/diffuse.png" alt=""></a>
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/hourlysolar.html" rel="permalink">시간별 태양광 발전량 계산 모델 수식 설명
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-11-25T00:00:00+09:00">2023-11-25</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">태양광 패널을 포함하는 마이크로그리드의 경제성 분석을 위해서는 시간별 태양광 발전량을 계산해야 한다. 필자가 국내 케이스의 계산에 사용하는 데이터로는 기상청에서 제공하는 시간별 일사량이 있다. 즉, 시간별 일사량 데이터를 태양광 발전량으로 변환해야 한다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <a href="/opf.html" rel="permalink"><img src="/assets/images/opf/network.png" alt=""></a>
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/opf.html" rel="permalink">Optimal power flow 예제 소개 문헌 및 풀이 코드
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-11-11T00:00:00+09:00">2023-11-11</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">필자의 ‘선형계획법 기반 분산에너지시스템 최적화’ 시리즈 글에서는, 유효전력의 수요-공급을 고려하는 설비도입계획을 다루었다. 그러나 기 구축된 대규모 발송전시스템의 실제 운영은, 각 발전기 및 bus의 전압/ 무효전력/ 위상도 고려하는 optimal power flow (OPF) ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/koreanmarketrules.html" rel="permalink">국내 전력시장 주요 운영규칙 핵심사항 정리
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-10-22T00:00:00+09:00">2023-10-22</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">최근 에너지 플랫폼 업계 현직자들과 대화를 하며, 국내 전력시장 운영규칙을 상세히 알 필요성을 느꼈다. 본격적으로 에너지 플랫폼 업계에 종사한다면 전력시장 입찰을 전제로 한 에너지 설비들의 데이터 및 비즈니스 모델을 주로 분석하게 될 것이라 생각했기 때문이다. (그간 필자는 BTM...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <a href="/evestimate.html" rel="permalink"><img src="/assets/images/evestimates/teaser.png" alt=""></a>
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/evestimate.html" rel="permalink">논문 소개: 전기차 충전 이력 데이터 기반의, 계통 내 시간별 전기차 충전 부하 시뮬레이션
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-10-21T00:00:00+09:00">2023-10-21</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">전기차 및 충전 인프라 규모가 증가할수록 전력계통에 걸리는 부하가 증가한다. 그러므로 향후 발전/ 송전/ 배전 (특히 배전단) 설비 및 스케줄링 계획 수립을 위해, 전기차 충전으로 인한 시간별 부하 증가를 정확히 예측할 필요가 있다.
</p>
  </article>
</div>

        
      </div>
    </div> -->
  
  <!--  -->
</div>

  </div>

  

  <aside class="sidebar__top">
    <a href="#site-nav"> <i class="fas fa-angle-double-up fa-2x"></i></a>
    </aside>

  <div id="footer" class="page__footer">
    <footer>
      <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
      <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Contact:</strong></li>
    

    
      
        
          <li><a href="mailto:song4energy@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://github.com/song4energyndata" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 송정훈. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

    </footer>
  </div>

  
  <script src="/assets/js/main.min.js"></script>










</body>

</html>