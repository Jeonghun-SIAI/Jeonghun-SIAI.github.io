<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">

<head>
  <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>강화학습 기반 마이크로그리드 control - 2) Q-learning 개념 | 에너지엔데이터연구소</title>
<meta name="description" content="지난 포스팅에서, Vincent의 태양광 기반 마이크로그리드의 누적 비용을 최소화하는 최적 control 문제를 소개했다. 또한 이를 선형계획법으로 풀 경우 ‘미래의 태양광 발전량과 부하를 안다’라는, ‘비현실적’인 가정 하의 control을 도출함을 보였다.">


  <meta name="author" content="Jeonghun Song">
  
  <meta property="article:author" content="Jeonghun Song">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="에너지엔데이터연구소">
<meta property="og:title" content="강화학습 기반 마이크로그리드 control - 2) Q-learning 개념">
<meta property="og:url" content="https://song4energyndata.github.io/reinforcetwo.html">


  <meta property="og:description" content="지난 포스팅에서, Vincent의 태양광 기반 마이크로그리드의 누적 비용을 최소화하는 최적 control 문제를 소개했다. 또한 이를 선형계획법으로 풀 경우 ‘미래의 태양광 발전량과 부하를 안다’라는, ‘비현실적’인 가정 하의 control을 도출함을 보였다.">



  <meta property="og:image" content="https://song4energyndata.github.io/assets/images/reinforcetwo/reinforcement_deep.png">





  <meta property="article:published_time" content="2023-06-10T00:00:00+09:00">





  

  


<link rel="canonical" href="https://song4energyndata.github.io/reinforcetwo.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Jeonghun Song",
      "url": "https://song4energyndata.github.io/"
    
  }
</script>






  <meta name="naver-site-verification" content="7dda6777a2acc897ba51668ac7c58c380307dd6c">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="에너지엔데이터연구소 Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<link rel="alternate" type="application/rss+xml" href="https://song4energyndata.github.io/feed.xml">
  <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<head>
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
</head>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- end custom head snippets -->
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BBZ8BXY4N7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BBZ8BXY4N7');
</script>
  <meta charset="utf-8">

<body
  class="layout--single">
  <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

  

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          에너지엔데이터연구소
          <span class="site-subtitle">스마트한 에너지 전환에 기여하는 데이터 사이언티스트</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/aboutme/">About Me</a>
            </li><li class="masthead__menu-item">
              <a href="/search/">Search</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tag</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">카테고리 목록</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


  <div class="initial-content">
    





<div id="main" role="main">
  

  <div class="sidebar sticky">
    
  
  
  
  
    
    
      
    
    
  
  
   <!-- three lines added by Jeonghun  -->
    

<nav class="nav__list">
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">카테고리 목록</label>
  <ul class="nav__items" id="category_tag_menu">
      <!--전체 글 수-->
      
      <li>
        <!--span 태그로 카테고리들을 크게 분류 ex) C/C++/C#-->
        <span class="nav__sub-title">Categories</span>
            <!--ul 태그로 같은 카테고리들 모아둔 페이지들 나열-->
            <ul>
                
                    
                
                    
                
                    
                        <li><a href="/optimalsystem" class="">Optimal system (11)</a></li>
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                <!--Cpp 카테고리 글들을 모아둔 페이지인 /categories/cpp 주소의 글로 링크 연결-->
                <!--category[1].size 로 해당 카테고리를 가진 글의 개수 표시--> 
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/energymanagement" class="">EMS (5)</a></li>
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/estimation" class="">Estimation (2)</a></li>
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/dataset" class="">Dataset (4)</a></li>
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                        <li><a href="/mathstat" class="">Math. & Stat. (2)</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                        <li><a href="/etc" class="">Etc. (2)</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
      </li>
  </ul>
</nav>
  
  

  </div>




  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="강화학습 기반 마이크로그리드 control - 2) Q-learning 개념">
    <meta itemprop="description" content="지난 포스팅에서, Vincent의 태양광 기반 마이크로그리드의 누적 비용을 최소화하는 최적 control 문제를 소개했다. 또한 이를 선형계획법으로 풀 경우 ‘미래의 태양광 발전량과 부하를 안다’라는, ‘비현실적’인 가정 하의 control을 도출함을 보였다.">
    <meta itemprop="datePublished" content="2023-06-10T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="https://song4energyndata.github.io/reinforcetwo.html" class="u-url" itemprop="url">강화학습 기반 마이크로그리드 control - 2) Q-learning 개념
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-06-10T00:00:00+09:00">2023-06-10</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header> 
              <ul class="toc__menu"><li><a href="#state-action-pair-별-누적-보상">State-action pair 별 누적 보상</a></li><li><a href="#누적-보상-추정-bellman-equation과-q-learning">누적 보상 추정: Bellman equation과 Q-learning</a></li><li><a href="#현재의-action-결정-exploration-exploitation">현재의 action 결정: Exploration/ Exploitation</a></li><li><a href="#state-action-pair-수가-매우-많은-경우-q-value-근사를-위한-딥러닝-사용">State-action pair 수가 매우 많은 경우: Q-value 근사를 위한 딥러닝 사용</a></li></ul>
 <!-- 우측 TOC -->
            </nav>
          </aside>
        
        <p>지난 포스팅에서, Vincent의 태양광 기반 마이크로그리드의 누적 비용을 최소화하는 최적 control 문제를 소개했다. 또한 이를 선형계획법으로 풀 경우 ‘미래의 태양광 발전량과 부하를 안다’라는, ‘비현실적’인 가정 하의 control을 도출함을 보였다.</p>

<p>이번 포스팅에서는 ‘매 시점별로 과거의 자료만을 갖고’ control하는 데 필요한 강화학습의 이론적 내용을 최대한 간단히 소개한다 (강화학습에 대한 상세 내용은 <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Sutton의 Introduction</a> 참고 바람)</p>

<p><br /></p>

<h2 id="state-action-pair-별-누적-보상">State-action pair 별 누적 보상</h2>
<p>먼저, 강화학습에서 가장 널리 쓰이는 방법인인 Q-learning을 간단히 설명한다.</p>

<p>어떤 환경에서 시점 t에 state $s_{t}$에 있고, 이 때 action $a_{t}$를 수행하면 reward $r_{t+1}$을 받으면서 ‘next’ state $s_{t+1}$로 전이된다고 하자.</p>

<p><img src="/assets/images/reinforceone/reinforcement_basic.png" alt="reinforcement_basic" class="align-center" width="70%" />
<em>강화학습 개요. State $S_t$에서 Action $A_t$를 취하면 Reward $R_{t+1}$을 얻으면서 State $S_{t+1}$로 전이됨.</em></p>

<p>Vincent의 마이크로그리드 사례에서는, 시점 $t$에서 state는 $t-N, t-(N-1), …, t-1$ 시점들의 태양광발전량과 부하 및 $t-1$시점의 배터리 내 에너지량들의 총 $2N+1$차원 벡터로 두었고, action은 net 수전량 $p_{\text{import}}[t]-p_{\text{export}}[t]$이다 (net 송전 시 마이너스). Reward는 계통으로 송전시의 수익 (양의 reward) 또는 계통으로부터 수전하거나 loss of load가 발생할 때의 비용 (음의 reward) 이다.</p>

<p>참고로 ESS의 충/방전은, 계통 수전/송전 값이 결정되면 에너지 밸런스를 맞추도록 ‘자동으로’ 결정된다고 가정한다. 이런 식으로 action의 자유도 (degree of freedom)를 줄여야 실제 계산이 용이해진다.</p>

<p><br />
각 state $s$에서 action $a$를 할 확률을 나타내는 policy $\pi(a \vert s)$가 주어졌다고 하자. 그리고 시점 $t$에서의 state $s_t$이고, 시점 $t$에서는 poilcy $\pi(a \vert s)$에 관계없이 특정 action $a_t$를 결정하되, 다음 시점인 $t+1$부터는 $\pi(a \vert s)$에 따라 action들 $a_{t+1}, a_{t+2}, \cdots$를 결정한다고 하자.</p>

<p>이 때, state-action pair $(s_t,a_t)$에 대해 ‘미래의 reward들을 합친 누적 보상(return)의 기대값’ $q_{\pi}(s_{t},a_{t})$는 다음과 같다.</p>

<p>$ q_{\pi}(s_{t},a_{t}) = \mathbb{E} [r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots] $</p>

<p>우변에 기대값 기호 $ \mathbb{E}$가 붙은 이유는, 일반적으로는 같은 $s_{t}$에서 같은 $a_{t}$를 수행하더라도, 그 결과인 $r_{t+1}$과 $s_{t+1}$는 그때그때 다를 수 있으며 (즉 random, stochastic일 수 있음), 이에 따라 $t+1$시점 이후의 보상과 상태 전이도 달라질 수 있기 때문이다. 마이크로그리드 케이스에서도, 태양광 발전량과 부하는 외생적으로 random하게 결정되며 이에 따라 수전/송전량이 결정되므로, reward도 random하게 결정된다.</p>

<p>$\gamma$는 1보다는 작되 1에 가까운 양수로 discount factor이다. 이를 곱하지 않으면, 환경에서의 행동이 특정 시점에 반드시 끝나는 게 아닌 한 누적비용이 무한대가 될 수 있다. 그러므로 discount factor를 곱해줘야 누적비용이 유한한 값으로 보장된다.</p>

<p><br /></p>

<h2 id="누적-보상-추정-bellman-equation과-q-learning">누적 보상 추정: Bellman equation과 Q-learning</h2>

<p>한편 시점 $t+1$에 대해서도 $q_{\pi}(s_{t+1},a_{t+1})$이 정의되므로, 위 식은 아래와 같이 $q_{\pi}$에 대해 재귀적인(recursive) 식으로 쓸 수 있다.</p>

<p>$ q_{\pi}(s_{t},a_{t}) = \mathbb{E} [r_{t+1} + \gamma q_{\pi}(s_{t+1},a_{t+1}) ] $</p>

<p>특히 주어진 policy가 ‘최적의’ 즉 optimal policy라면, 시점 $t+1$부터의 action들은 미래 누적비용의 기대값을 최대화도록 선택될 것이다. 따라서 ‘optimal policy 조건 하의’ 누적 보상의 기대값 $q_{\ast}$에 대해, $\text{max}$를 포함하는 아래 식이 성립한다.</p>

<p>$ q_{\ast} (s_{t},a_{t}) = \mathbb{E} [r_{t+1} + \gamma \text{max}_{a_{t+1}} q_{\ast} (s_{t+1},a_{t+1}) ] $</p>

<p>이를 Bellman equation이라 한다.</p>

<p><br />
우리는 optimal policy 및 각 state-action pair 별 $q_{\ast}(s,a)$ 값을 모른다. 그러므로 처음에는 특정 policy 및 $q_{\ast}(s,a)$에 대한 추정량 $Q(s,a)$의 초기값으로부터 출발한다. 그리고 해당 policy 및 $Q(s,a)$를 업데이트시켜서, 점차 optimal policy 및 $q_{\ast}(s,a)$에 근접하게 만들어야 한다.</p>

<p>Bellman equation의 식을 보면, 우변에서 next state에 대해 $q_{\ast}$ 값을 최대화하는 action을 선택함이 전제되어 있다. 이에 착안해, 아래와 같은 $Q(s,a)$의 업데이트 방법을 생각할 수 있다.</p>

<p>$ Q(s_{t},a_{t}) \leftarrow (1-\alpha) Q(s_{t},a_{t}) + \alpha [r_{t+1} + \gamma \text{max}_{a_{t+1}} Q(s_{t+1},a_{t+1})] $</p>

<p>즉 시점 $t$에서 state $s_t$일 때 action $a_t$를 수행하여 reward $r_{t+1}$을 얻고 next state $s_{t+1}$로 전이되었다면, $s_{t+1}$에서 가능한 action들 중 추정량 $Q(s_{t+1},a_{t+1})$를 최대화하는 action $a_{t+1}$을 선택한다고 가정하고 $Q(s_{t},a_{t})$를 업데이트한다. (여기서 $\alpha$는 작은 양수로, learning rate이다)</p>

<p>처음에는 모든 state-action pair들에 대해 $Q(s,a)$를 0으로 초기화하고, 시점 $t$에서 action을 결정해서 $t+1$로 넘어갈 때마다 위와 같이 업데이트 한다.</p>

<p>처음에는 모든 $Q$값이 0이므로 각 state $s$ 별로 $Q(s,a)$를 최대화하는 $a$가 무작위이다. 그러나 업데이트를 수행하면서 ‘실제 환경에서 얻은 정보’인 reward $r_{t+1}$ 를 계속 반영하므로, 업데이트를 거듭할수록 $Q(s,a)$가 점차 $ q_{\ast} (s,a)$에 가까워짐이 알려져 있다. 이에 따라, $Q(s,a)$를 최대화하는 $a$가 무엇인지 뚜렷해지므로 policy도 업데이트되는 셈이다.</p>

<p>이러한 업데이트 방법을 Q-learning이라 한다.</p>

<p><br />
Vincent의 마이크로그리드 사례에서는, 실제 수전량은 -1.1에서 +1.1 사이의 실수이지만 (마이너스값은 송전), 문제를 간단히 하기 위해 세 가지의 ‘discrete’ action들로 구성된 action set (1.1kW로 수전, 1.1kW로 송전, 수전도 송전도 하지 않음 (idle)) 을 가정했다.</p>

<p>위 세 가지 action 각각의 각 action의 인덱스를 0,1,2라 하자. 그리고 시점 $t$에서 상태가 $s_t$일 때 3개의 action들 중 0번 action을 $a_t$로써 정해서 reward $r_{t+1}$을 얻은 후 다음 상태 $s_{t+1}$로 전이되었다고 하자.</p>

<p>그러면 현재의 추정량들 $Q(s_t,0)$, $Q(s_{t+1},0)$, $Q(s_{t+1},1)$, $Q(s_{t+1},2)$를 사용해서, 추정량 $Q(s_t,0)$를 업데이트할 수 있다. 이를 시점 $t, t+1, \cdots$ 에 대해 계속 반복한다.</p>

<p><br /></p>

<h2 id="현재의-action-결정-exploration-exploitation">현재의 action 결정: Exploration/ Exploitation</h2>

<p>위에서 $q_{\pi}(s_t,a_t)$를 설명할 때, 시점 $t+1$부터는 policy $\pi$를 따라 action들 $a_{t+1}, a_{t+2}, \cdots$를 결정하되, 시점 $t$에서는 $\pi$에 관계없이 action $a_t$를 결정한다고 했다.</p>

<p>그러면, $a_t$는 어떻게 결정해야 하는가?</p>

<p>가장 직관적인 방법은, 시점 $t$의 state가 $s_t$로 주어졌을 때 $Q(s_t,a_t)$를 최대화하는 action으로 결정하는 것이다. 이러한 $a_t$ 결정 규칙을 ‘greedy’ policy라 한다.</p>

<p>그러나 greedy policy는 controller 훈련을 끝낸 후 ‘validation 및 실제 구동 시’에 적용해야지, ‘훈련 중’에 적용하는 것은 적절하지 않다.</p>

<p>학습 초기에 아직 아무 정보도 학습하지 못했는데 단순히 $Q$ 값이 가장 큰 action만을 계속 하는 것은, 마치 평균적 보상이 다른 여러 개의 슬롯머신을 눈 앞에 두고도, 처음에 고른 몇 개의 슬롯머신 중 좋아 보이는 것만 계속 누르는 것과 비슷하다.</p>

<p>누적 보상을 최대화하려면 어떤 슬롯머신이 가장 (평균적으로) 큰 보상을 주는지 확인하는 절차가 먼저 필요하며, 이를 위해 초반에는 여러 슬롯머신들을 돌아가면서 눌러봐야 한다. 강화학습에서는 이를 exploration(탐색)이라 한다.</p>

<p>탐색을 통해 어떤 슬롯머신이 보상을 많이 주는지 감이 잡히면 그때부터 보상을 많이 주는 슬롯머신을 계속 누르면 되며, 이를 exploitation이라 한다. Q-learning에서 greedy policy는 exploitation’만’ 하는 것에 해당한다.</p>

<p>훈련 시에는 (특히 초반에는) exploration을 겸해서 해 줘야 더 좋은 action을 판별할 가능성이 열린다. 그러므로 시점 $t$에서 $a_t$를 결정할 때, 보통 주어진 값 $\epsilon$만큼의 확률로 $a_t$를 $Q(s_t,a_t)$ 값에 상관없이 random하게 결정한다 (exploration). 그리고 $1-\epsilon$의 확률로 $Q(s_t,a_t)$ 값이 최대가 되는 $a_t$를 결정한다 (exploitation).</p>

<p>이렇게 exploration과 exploitation 간 balance를 갖춘 $a_t$ 결정 규칙을  $\epsilon$-greedy policy라 한다. ($\epsilon$는 훈련 내내 고정되어야 하는 것은 아니며, 훈련 초기에는 크게 하고 훈련이 진행될수록 줄여서 적용할 수도 있다.)</p>

<p>(Exploration/ exploitation에 대해 제대로 이해하고 싶으면, Sutton의 책 2장의 ‘Multi-armed Bandits’를 공부하기 바란다)</p>

<p><br /></p>

<h2 id="state-action-pair-수가-매우-많은-경우-q-value-근사를-위한-딥러닝-사용">State-action pair 수가 매우 많은 경우: Q-value 근사를 위한 딥러닝 사용</h2>
<p>지금까지 설명한 방법은, state-action pair의 수가 많지 않을 경우 각 state-action pair별 $Q$값들을 담는 table을 만들어 그대로 적용할 수 있다.</p>

<p>이를테면 아래와 같은 공간에서 상하좌우로 한 칸씩 이동 가능한 환경에서 최단거리를 찾는 문제 (Gridworld) 에서는, 격자 수가 한정되어 있으므로 각 격자(state)-이동방향(action) pair 별로 $Q$값들을 따로 계산할 수 있다.</p>

<p><img src="/assets/images/reinforcetwo/gridworld.png" alt="gridworld" class="align-center" />
<em>Gridworld 환경. 칸이 38개이고 action은 상하좌우 4개이므로, state-action pair 총 152개 각각에 대해 Q-value를 추정할 수 있다.</em></p>

<p>하지만 많은 경우 state가 continuous이거나, discrete라도 그 수가 매우 많다.</p>

<p>잘 알려진 cartpole 문제의 경우, action은 ‘수레를 정해진 힘으로 왼쪽으로 가속/ 오른쪽으로 가속’ 딱 두 개의 값 중 하나를 가지는 이진수로 볼 수 있으나, state는 ‘수레의 위치와 속도, 막대의 각도와 각속도’로 4차원의 연속된 벡터로 볼 수 있다. 연속변수이므로, state의 수가 ‘무한대’이다. 그러므로, 각 state-action pair에 대해 table 방식으로 $Q$값들을 따로 계산할 수는 없다.</p>

<p><img src="/assets/images/reinforcetwo/cart_pole.gif" alt="cart_pole" class="align-center" width="70%" />
<em>Cartpole 환경. State 수가 무한하므로, 각 state-action pair에 대해 Q-value를 정의할 수 없다.</em></p>

<p>Vincent의 마이크로그리드 케이스에서도 마찬가지이다. State (직전 $N$시간 동안의 시간별 부하와 태양광 발전량, 그리고 배터리에 저장된 에너지)가 $2N+1$차원의 continuous vector이기 때문이다.</p>

<p>이런 경우 ‘모든 state-action pair 각각에 대해’ $Q$를 계산한다는 것은 사실상 불가능하다. 그렇다면 어떻게 해야 하나?</p>

<p><br />
만약 state-action pair와 Q값 간의 관계가 어떤 domain knowledge에 의해 명확하게 정의될 수 있다면, 해당 관계를 명확한 수식으로 표현하여 $Q$를 쉽게 계산할 수 있을 것이다.</p>

<p>그러나 이 케이스를 포함한 많은 경우, state-action과 $Q$값 간의 관계식을 명확한 수식으로 나타낼 수 있을 것이라 기대되지 않는다.</p>

<p><br />
그렇다면, state $s_t$를 입력으로 받고 각 action 별 $Q(s_t,a_t)$ 값들을 출력으로 하는 심층신경망 (Deep Neural Network, DNN) 모델을 훈련하는 것이 현실적인 방법이다.</p>

<p>심층신경망은 계산비용은 매우 높지만, node 수가 충분하면 어떤 비선형 함수도 근사할 수 있기 때문에 nonlinear function approximator로 기능한다는 점을 이용하는 것이다.</p>

<p><img src="/assets/images/reinforcetwo/reinforcement_deep.png" alt="reinforcement_deep" class="align-center" width="80%" />
<em>‘심층’강화학습 개요. Action 결정이 심층신경망에 기반해 이루어짐.</em></p>

<p>이 심층신경망 모델은 매 훈련 주기마다 tuple $(s_{t},a_{t},s_{t+1},r_{t+1})$ 의 batch를 받아서 업데이트된다. 이 때 훈련 주기는 1 time step보다 길 수 있으며, batch라 함은 여러 시점들의 tuple들을 한꺼번에 훈련에 사용함을 의미한다.</p>

<p>업데이트 시 최소화 대상 목적함수는 $Q(s_{t},a_{t})$와 $r_{t+1} + \gamma \text{max}_{a_{t+1}} Q(s_{t+1},a_{t+1})$ 간 차이의 평균제곱이다. Gradient descent 기반으로 심층신경망의 가중치를 수정해감에 따라, $Q(s_{t},a_{t})$가 $q_{\ast}(s_{t},a_{t})$에 가까워질 것이라 기대할 수 있다.</p>

<p>이를 Deep Q-Network (DQN) 이라 한다. (위 cartpole 문제도 DQN으로 푼다)</p>

<p><br />
다음 포스팅에서는 마이크로그리드 문제에 Deep Q-Network를 적용하는 과정과 결과에 대해 상세히 설명한다.</p>

<div class="notice--info">

강화학습 기반 마이크로그리드 control<br /><br />

1) <a href="/reinforceone.html">문제의식 및 케이스 소개</a><br />
2) <b>Q-learning 개념</b><br />
3) <a href="/reinforcethree.html">Deep Q-Network를 통한 discrete control 도출</a><br />
4) <a href="/reinforcefour.html">DDPG를 통한 continuous control 도출</a><br />
5) <a href="/reinforcefive.html">TD3, SAC를 통한 continuous control 도출</a>

</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5" class="page__taxonomy-item p-category" rel="tag">강화학습</a>
    
    </span>
  </p>



<!-- 
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#energymanagement" class="page__taxonomy-item p-category" rel="tag">energymanagement</a>
    
    </span>
  </p>

 -->
        <!-- 

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2023-06-10T00:00:00+09:00">2023-06-10</time></p>
 -->
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <!-- <a href="https://twitter.com/intent/tweet?text=%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5+%EA%B8%B0%EB%B0%98+%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EA%B7%B8%EB%A6%AC%EB%93%9C+control+-+2%29+Q-learning+%EA%B0%9C%EB%85%90%20http%3A%2F%2Flocalhost%3A4000%2Freinforcetwo.html" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> -->

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Freinforcetwo.html" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Freinforcetwo.html" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      <p></p>
        <!-- <h4 class="page__meta-title"><span>energymanagement</span> <span>카테고리 내 다른 글 보러가기</span></h4> -->
        


  

  

  

  
  	
  	
  	
  	
  	

<nav class="pagination_prev_next"> <!-- 식빵맘 코드에서 조금 수정함 -->

  
    
      <a href="/reinforcethree.html" class="pagination_prev_next--pager"><span class="prev_next">다음 글  &nbsp  </span>강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 discrete control 도출</a>
    
    
      <a href="/reinforceone.html" class="pagination_prev_next--pager"><span class="prev_next">이전 글  &nbsp</span>강화학습 기반 마이크로그리드 control - 1) 문제의식 및 케이스 소개</a>
        
  

</nav>
      <p></p>
    </div>

    
  </article>

  
  <!-- 
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/reinforcefive/result_td3sac.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reinforcefive.html" rel="permalink">강화학습 기반 마이크로그리드 control - 5) TD3/ SAC를 통한 continuous control 도출
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-07-01T00:00:00+09:00">2023-07-01</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Deep Deterministic Policy Gradient (DDPG) 로 도출한 수전/송전의 continuous control이, 놀랍게도(?) Vincent의 마이크로그리드 사례에서는, DQN으로 도출한 3-actions discrete control 대비 더 좋지 않았다 ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/reinforcefour/result_ddpg.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reinforcefour.html" rel="permalink">강화학습 기반 마이크로그리드 control - 4) DDPG를 통한 continuous control 도출
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-06-25T00:00:00+09:00">2023-06-25</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Vincent의 마이크로그리드 사례에 ‘discrete’ action (1.1kW 수전/ 1.1kW 송전/ idle) 기반의 Deep Q-Network (DQN) 을 적용한 결과, 미래를 모른 채 과거 24시간 동안의 태양광 발전량과 부하 정보를 활용하더라도 충분히 economic...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/reinforcethree/nn.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reinforcethree.html" rel="permalink">강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 discrete control 도출
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-06-11T00:00:00+09:00">2023-06-11</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Vincent의 마이크로그리드 사례에서 Q-learning의 concept를 이용하기 위해, 실제로는 수전/송전이 continuous한 값임에도 불구하고, 1.1kW 수전/ 1.1kW 송전/ idle 의 3가지 action만을 고려하기로 했다. 각 action 별 인덱스는 0, 1...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/overconfidence.html" rel="permalink">로지스틱 회귀에서의 over-confidence에 대한 이해
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-05-28T00:00:00+09:00">2023-05-28</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Logistic regression에서는 각 point 별로 response $y$가 0과 1 중 1일 확률 추정값 $\hat{y}$를 제공한다 (물론 실제 $y$값들은 알려져 있다). 한편, feature space에서 response가 1인 point들과 0인 point들이 완...</p>
  </article>
</div>

        
      </div>
    </div> -->
  
  <!--  -->
</div>

  </div>

  

  <aside class="sidebar__top">
    <a href="#site-nav"> <i class="fas fa-angle-double-up fa-2x"></i></a>
    </aside>

  <div id="footer" class="page__footer">
    <footer>
      <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
      <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Contact:</strong></li>
    

    
      
        
          <li><a href="mailto:song4energy@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://github.com/song4energyndata" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Jeonghun Song. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

    </footer>
  </div>

  
  <script src="/assets/js/main.min.js"></script>










</body>

</html>