<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-10T11:56:45+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">A test page</title><subtitle>GitHub Page를 익히기 위한 테스트 페이지입니다</subtitle><author><name>Jeonghun Song</name></author><entry><title type="html">회귀분석 관련 정리 2</title><link href="http://localhost:4000/2023/04/regressiontwo.html" rel="alternate" type="text/html" title="회귀분석 관련 정리 2" /><published>2023-04-09T00:00:00+09:00</published><updated>2023-04-09T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/regressiontwo</id><content type="html" xml:base="http://localhost:4000/2023/04/regressiontwo.html"><![CDATA[<h2 id="ordinary-least-squares">Ordinary least squares</h2>

<ul>
  <li>
    <p>선형회귀모델 $y=X \beta + \epsilon$ 에는 아래의 다섯가지 가정이 들어감.</p>

    <p>A1:  $X$는 full rank임 (즉 특정 설명변수를 다른 설명변수들의 선형결합으로 완전하게 나타낼 수 없으며, $X’X$의 역행렬이 존재). 이 조건이 깨지면 estimator를 구할 수 없음.</p>

    <p>A2: 설명변수와 반응변수 간에 선형관계가 성립하고, 오차항의 편향이 없음 (즉 $\text{E}[\epsilon] = 0$).</p>

    <p>A3: 설명변수 $X$와 오차항 $\epsilon$ 간에는 상관관계가 없음. $X$가 fixed된 값들이거나 (A3F) random이더라도 $\epsilon$과 완전히 독립인 경우 (A3Rfi), 또는 random인 $X$가 전부 주어졌을 때 오차항의 조건부평균 $\text{E}[ \epsilon_{t} | X] = 0$ 이면 (A3Rmi), estimator는 unbiased이면서 consistent함. 만약 $x_i$가 적어도 같은 row의 $\epsilon_{i}$와는 상관이 없다면 (A3Rsru), estimator는 biased일 수는 있으나 consistent함 (이를 테면 시계열에서 시점 $t$의 설명변수는, 과거의 오차항과는 상관이 있으나 현재 및 미래의 오차항과는 상관이 없음.</p>

    <p>A4: 오차항은 서로 독립이고 같은 분포를 따름, 즉 상관되어 있지 않고 일정한 분산을 가짐 (즉 $\text{E}[\epsilon \epsilon’ | X] = \sigma^2 I_N$). 이 조건이 깨지면 Generalized least square가 필요.</p>

    <p>A5: 오차항이 정규분포를 따름. A5가 만족될 경우 MLE estimator가 OLS estimator와 같으므로 OLS가 효율적임. 꼭 정규분포는 아니라도 오차항의 분포를 알아야 MLE를 쓸 수 있음.</p>
  </li>
  <li>
    <p>OLS에서 목적함수는 반응변수의 실제값과 추정값(설명변수 및 선형모델로 계산된) 간 차이의 제곱합인 $\sum_{i=1}^N [(y_i - x_i’ \beta)^2 ] = (y-X\beta)’(y-X\beta)$ 이며, 이를 최소화하는 해는 $\hat{\beta}_{OLS}=(X’X)^{-1}X’y$ 임.</p>
  </li>
  <li>
    <p>Vector space 관점에서 보면, $X\hat{\beta}_{OLS}$는 $X$의 열공간의 원소이므로 $X\hat{\beta}_{OLS} = X(X’X)^{-1}X’y = \hat{y}$ 는 $y$를 $X$의 열공간에 정사영한 벡터, 즉 $X$의 열공간에 속하는 벡터 중 $y$에 가장 가까운 벡터임. 이로부터 $y$를 $X$의 열공간 내의 벡터로 변환하는 사상에 대응되는 행렬 $X(X’X)^{-1}X’$를 hat matrix라 하며, $y$의 정보 중 $X$로 설명되는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>A3Rsru 조건 $\text{E}[x_i \epsilon_i] = 0$ 으로부터 sample moment condition $\sum_{i=1}^N x_i (y_i - x_i’ \beta)/N = 0$ 을 세워 계산한 method of moments estimator $\hat{\beta}_{MME}$ 는 $\hat{\beta}_{OLS}$와 같음.</p>
  </li>
  <li>
    <p>$y$와 $\hat{y}$ 간 차이는 잔차(residual, $\hat{\epsilon}$)임. 잔차벡터를 식으로 나타내면 $\epsilon = y - \hat{y} = (I-X(X’X)^{-1}X’)y$ 임. 행렬 $I-X(X’X)^{-1}X’$를 projection matrix라 하며, $y$의 정보 중 $X$로 설명되지 않는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>OLS에서의 잔차는 실제 오차와 다른 개념임. 실제로는 오차가 A4GM 가정을 만족하지 않더라도 OLS 잔차는 A4GM이 만족됨을 가정하고 계산된 값임. 그러므로 반드시 잔차 plot을 그려서 이분산이나 오차의 자기상관 등이 의심되는지를 확인해야 함. 또한 오차가 설명변수와 상관이 있든 없든, OLS 계산 결과인 잔차는 항상 $X$와 직교함 ($X \hat{\epsilon} = 0$). 그러므로 A3가정의 만족 여부는 잔차만으로는 알 수 없으며 endogeneity 관련 방법론이 필요함.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi가 만족되면, $\text{E}[\hat{\beta}_{OLS} | X] = \beta$, 즉 OLS estimator는 unbiased임 ($y=X\beta+\epsilon$으로부터 유도). 또한 OLS estimator의 공분산행렬을 계산하면 $\text{Var}[\beta|X] = \sigma^2 (X’X)^{-1}$ 임 ($\hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon$ 으로부터 유도).</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 unbiased linear estimator ($\text{E}[\tilde{\beta}] = \beta$ 즉 unbiased 이면서, 어떤 full rank 행렬 A ($AX=I$) 에 대해 $\tilde{\beta} = Ay$ 즉 $y$에 대해 linear인 임의의 estimator) 들 중 분산이 가장 작은 (즉 가장 효율적인) estimator임. 이를 Gauss-Markov theorem이라 함.</p>
  </li>
  <li>
    <p>Gauss-Markov theorem에서는 A5 (오차항의 정규분포 가정) 가 필요하지 않았음에 주의. 또한 biased되어 있거나 nonlinear인 estimator 중에서는 $\hat{\beta}_{OLS}$ 대비 분산이 더 작은 estimator가 존재할 수 있음 (이를 테면 $X$에 multicollinearity 문제가 있을 경우 이를 완화하기 위한 Ridge 회귀의 estimator는 biased estimator이지만 분산은 더 작음).</p>
  </li>
  <li>
    <p>A1, A2, A3Rsru, A4가 만족되면, 표본크기가 무한히 많아질 때 OLS estimator는 true $\beta$에 확률수렴함 (확률수렴이란 말은, $X$의 DGP를 고려 시 엄연히 확률변수인 $\hat{\beta}_{OLS}$가, $N$이 무한히 커질 경우 true $\beta$에 매우 가까운 값으로 나올 확률이 100%임을 의미). 
이는 $ \hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon = (X’X/N)^{-1} (X’\epsilon/N) $ 에 A3Rsru 조건 만족 시 $\text{plim} X’\epsilon/N=0$, 그리고 Slutsky theorem으로부터 $\text{plim}(X’X)^{-1}X’\epsilon$을  $\text{plim}(X’X/N)^{-1}=\Sigma_{xx}^{-1}$와 $\text{plim}X’\epsilon/N=0$의 곱으로 나타낼 수 있다는 사실로부터 유도됨.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 asymptotically normal, 즉 표본크기가 크다면 따로 A5 가정을 하지 않아도 근사적으로 정규분포를 따름. $X’\epsilon/N$을 $\epsilon_i$의 표본평균에 $X’$가 곱해진 것으로 해석하면, 중심극한정리에 의해 $\sqrt{N} (X’\epsilon/N - 0) \rightarrow N(0,\sigma^2 \Sigma_{xx})$ 이고 $\sqrt{N} ((X’X/N)^{-1}X’\epsilon/N - 0) = \sqrt{N}(\hat{\beta}_{OLS}-\beta) \rightarrow N(0,\sigma^2 \Sigma_{xx}^{-1})$ 이기 때문.</p>
  </li>
  <li>
    <p>실제로는 오차분산 $\sigma^2$를 정확히 알지 못하므로 표본 기반으로 추정함. A1, A2, A3Rmi, A4 조건이 만족될 경우, $s^2 = \sum_{i=1}^N \hat{\epsilon}^2/(N-k)$ 는 $\sigma^2$의 unbiased and consistent estimator임 ($k$는 intercept를 포함한 parameter의 수). 또한 A5 조건까지 만족될 경우, $(N-k) s^2/\sigma^2 \sim \chi^2(N-k)$ 임.</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 설명변수의 group 기준으로 분해해서 $y = X_1 \beta_1 + X_2 \beta_2 + \epsilon$ 으로 볼 때, $\beta_1$은 $X_2$가 통제된 상태에서 $X_1$만의 변화에 따른 $y$의 변화를 나타내야 함. 그러므로 $\hat{\beta}_1$을 구할 때는, $X_1$로부터 $X_2$의 정보를 projection matrix를 곱해 제거한 $(I - X_2(X_2’X_2)^{-1}X_2’)X_1 = M_2 X_1$ 을 기반으로 회귀해 구해야 함. 즉 $\hat{\beta}_1 = (X_1’M_2 X_1)^{-1} X_1’ M_2 y$ 임 (projection matrix가 idempotent, 즉 $M_2^2 = M_2$임을 이용함).</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 위에서는 $X$의 column을 기준으로 분해했는데, 이와 달리 모델을 row 기준으로 분해할 경우 (이를테면 특정 행 기준으로 old와 new로 구분할 경우), $X’X \hat{\beta} = X’y$ 로부터 아래와 같이 됨. 
$ \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      X_{old} \ 
      X_{new}
  \end{bmatrix}<br />
  \hat{\beta} = 
  \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      y_{old} \ 
      y_{new}
  \end{bmatrix}<br />
  \, \Rightarrow \, [X_{old}’X_{old} + X_{new}’X_{new}] \hat{\beta} = X_{old}’ y_{old} + X_{new}’ y_{new} $
  이를 데이터의 갱신으로 보면 $X_{old}’X_{old}$ 및 $(X_{old}’X_{old})^{-1}$ 가 계산되어 있는 상황이므로, 새로운 데이터 행렬 전체에 대해 새로 계산할 필요 없이 아래 식으로부터 $\hat{\beta}$를 빠르게 계산할 수 있음.
  $ [X_{old}’X_{old} + X_{new}’X_{new}]^{-1} = (X_{old}’X_{old})^{-1} - (I+ X_{new} (X_{old}’X_{old})^{-1} X_{new}’ )^{-1} (X_{old}’X_{old})^{-1} X_{new}’X_{new} (X_{old}’X_{old})^{-1} $</p>
  </li>
</ul>

<h2 id="maximum-likelihood-estimation-in-linear-regression">Maximum likelihood estimation in linear regression</h2>

<ul>
  <li>
    <p>A1부터 A5까지의 가정들이 성립 시, $y_i = x_i’\beta + \epsilon_i$는 $x_i, \beta, \sigma^2$가 주어질 경우 오차항이 iid이며 정규분포를 따른다는 가정으로부터 $y_i | x_i, \beta, \sigma^2 \sim N(x_i’ \beta, \sigma^2)$ 임. 그러므로 $y$ 전체의 likelihood 및 log-likelihood는 아래와 같음.
$ L(y_i | x_i, \beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{(y_i - x_i’\beta)^2}{2 \sigma^2} \right)  = \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) \right)   $
$ \text{log} L(y_i | x_i, \beta, \sigma^2) = - \frac{N}{2} \text{log}(2 \pi) - \frac{N}{2} \text{log}(\sigma^2) - \frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) $
여기서 log-likelihood에 $-(y-X\beta)’(y-X\beta)$ 가 포함되어 있으므로, $\sigma^2$가 주어질 때 log-likelihood를 최대화하는 추정량 $\hat{\beta}_{MLE}$는 $(y-X\beta)’(y-X\beta)$를 최소화하는 추정량, 즉 OLS 추정량과 같음.</p>
  </li>
  <li>
    <p>$\beta$가 주어질 때 log-likelihood를 최대화하는 $\sigma^2$의 추정량은 $\widehat{\sigma^2}_{MLE} = \hat{\epsilon}’ \hat{\epsilon}/N$, 즉 잔차제곱합을 표본 크기로 나눈 값이 됨. 이는 biased estimator임 (OLS에서의 unbiased estimator는 잔차제곱합을 $N-k$로 나눈 값이었음). 그러나 표본크기가 무한히 커지면 $N-k$와 $N$이 비슷해지므로, $\sigma^2$의 maximum likelihood 추정량은 consistent함.</p>
  </li>
  <li>
    <p>MLE에서 parameter를 $\theta$라 할 때 $\theta$의 분산은 log-likelihood의 Hessian의 평균에 마이너스를 붙인 후 역행렬을 취한 $(-\text{E}[H(\theta)])^{-1}$로 주어짐. 이 때 $(-\text{E}[H(\theta)])$ 를 information matrix $I(\theta)$라 부르기도 하며, $\text{Var}[\theta] = I(\theta)^{-1}$ 임. 
이에 따라 계산하면 $\text{Var}[\hat{\beta}_{MLE}] = \sigma^2(X’X)^{-1}$을 얻고 (OLS estimator의 공분산행렬과 같음), $\text{Var}[\widehat{\sigma^2}_{MLE}] = 2\sigma^4/N$을 얻음.</p>
  </li>
  <li>
    <p>MLE로 추정한 추정량은 consistent하며, asymptotic normal이고, Consistent Uniformly Asymptotically Normal (CUAN) 성질을 갖는 estimator들 중 가장 효율적인 추정량임 (MLE 추정량의 공분산행렬은 CUAN estimator들의 Cramer-Rao lower bound, 즉 unbiased estimator가 가지는 분산의 하한임). 그러므로 오차항의 분포에 확신이 있다면, MLE를 쓰는 것이 가장 좋음.</p>
  </li>
</ul>]]></content><author><name>Jeonghun Song</name></author><category term="Theory" /><summary type="html"><![CDATA[Ordinary least squares]]></summary></entry><entry><title type="html">회귀분석 관련 정리 3</title><link href="http://localhost:4000/2023/04/regressionthree.html" rel="alternate" type="text/html" title="회귀분석 관련 정리 3" /><published>2023-04-09T00:00:00+09:00</published><updated>2023-04-09T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/regressionthree</id><content type="html" xml:base="http://localhost:4000/2023/04/regressionthree.html"><![CDATA[<h2 id="ordinary-least-squares">Ordinary least squares</h2>

<ul>
  <li>
    <p>선형회귀모델 $y=X \beta + \epsilon$ 에는 아래의 다섯가지 가정이 들어감.</p>

    <p>A1:  $X$는 full rank임 (즉 특정 설명변수를 다른 설명변수들의 선형결합으로 완전하게 나타낼 수 없으며, $X’X$의 역행렬이 존재). 이 조건이 깨지면 estimator를 구할 수 없음.</p>

    <p>A2: 설명변수와 반응변수 간에 선형관계가 성립하고, 오차항의 편향이 없음 (즉 $\text{E}[\epsilon] = 0$).</p>

    <p>A3: 설명변수 $X$와 오차항 $\epsilon$ 간에는 상관관계가 없음. $X$가 fixed된 값들이거나 (A3F) random이더라도 $\epsilon$과 완전히 독립인 경우 (A3Rfi), 또는 random인 $X$가 전부 주어졌을 때 오차항의 조건부평균 $\text{E}[ \epsilon_{t} | X] = 0$ 이면 (A3Rmi), estimator는 unbiased이면서 consistent함. 만약 $x_i$가 적어도 같은 row의 $\epsilon_{i}$와는 상관이 없다면 (A3Rsru), estimator는 biased일 수는 있으나 consistent함 (이를 테면 시계열에서 시점 $t$의 설명변수는, 과거의 오차항과는 상관이 있으나 현재 및 미래의 오차항과는 상관이 없음.</p>

    <p>A4: 오차항은 서로 독립이고 같은 분포를 따름, 즉 상관되어 있지 않고 일정한 분산을 가짐 (즉 $\text{E}[\epsilon \epsilon’ | X] = \sigma^2 I_N$). 이 조건이 깨지면 Generalized least square가 필요.</p>

    <p>A5: 오차항이 정규분포를 따름. A5가 만족될 경우 MLE estimator가 OLS estimator와 같으므로 OLS가 효율적임. 꼭 정규분포는 아니라도 오차항의 분포를 알아야 MLE를 쓸 수 있음.</p>
  </li>
  <li>
    <p>OLS에서 목적함수는 반응변수의 실제값과 추정값(설명변수 및 선형모델로 계산된) 간 차이의 제곱합인 $\sum_{i=1}^N [(y_i - x_i’ \beta)^2 ] = (y-X\beta)’(y-X\beta)$ 이며, 이를 최소화하는 해는 $\hat{\beta}_{OLS}=(X’X)^{-1}X’y$ 임.</p>
  </li>
  <li>
    <p>Vector space 관점에서 보면, $X\hat{\beta}_{OLS}$는 $X$의 열공간의 원소이므로 $X\hat{\beta}_{OLS} = X(X’X)^{-1}X’y = \hat{y}$ 는 $y$를 $X$의 열공간에 정사영한 벡터, 즉 $X$의 열공간에 속하는 벡터 중 $y$에 가장 가까운 벡터임. 이로부터 $y$를 $X$의 열공간 내의 벡터로 변환하는 사상에 대응되는 행렬 $X(X’X)^{-1}X’$를 hat matrix라 하며, $y$의 정보 중 $X$로 설명되는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>A3Rsru 조건 $\text{E}[x_i \epsilon_i] = 0$ 으로부터 sample moment condition $\sum_{i=1}^N x_i (y_i - x_i’ \beta)/N = 0$ 을 세워 계산한 method of moments estimator $\hat{\beta}_{MME}$ 는 $\hat{\beta}_{OLS}$와 같음.</p>
  </li>
  <li>
    <p>$y$와 $\hat{y}$ 간 차이는 잔차(residual, $\hat{\epsilon}$)임. 잔차벡터를 식으로 나타내면 $\epsilon = y - \hat{y} = (I-X(X’X)^{-1}X’)y$ 임. 행렬 $I-X(X’X)^{-1}X’$를 projection matrix라 하며, $y$의 정보 중 $X$로 설명되지 않는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>OLS에서의 잔차는 실제 오차와 다른 개념임. 실제로는 오차가 A4GM 가정을 만족하지 않더라도 OLS 잔차는 A4GM이 만족됨을 가정하고 계산된 값임. 그러므로 반드시 잔차 plot을 그려서 이분산이나 오차의 자기상관 등이 의심되는지를 확인해야 함. 또한 오차가 설명변수와 상관이 있든 없든, OLS 계산 결과인 잔차는 항상 $X$와 직교함 ($X \hat{\epsilon} = 0$). 그러므로 A3가정의 만족 여부는 잔차만으로는 알 수 없으며 endogeneity 관련 방법론이 필요함.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi가 만족되면, $\text{E}[\hat{\beta}_{OLS} | X] = \beta$, 즉 OLS estimator는 unbiased임 ($y=X\beta+\epsilon$으로부터 유도). 또한 OLS estimator의 공분산행렬을 계산하면 $\text{Var}[\beta|X] = \sigma^2 (X’X)^{-1}$ 임 ($\hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon$ 으로부터 유도).</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 unbiased linear estimator ($\text{E}[\tilde{\beta}] = \beta$ 즉 unbiased 이면서, 어떤 full rank 행렬 A ($AX=I$) 에 대해 $\tilde{\beta} = Ay$ 즉 $y$에 대해 linear인 임의의 estimator) 들 중 분산이 가장 작은 (즉 가장 효율적인) estimator임. 이를 Gauss-Markov theorem이라 함.</p>
  </li>
  <li>
    <p>Gauss-Markov theorem에서는 A5 (오차항의 정규분포 가정) 가 필요하지 않았음에 주의. 또한 biased되어 있거나 nonlinear인 estimator 중에서는 $\hat{\beta}_{OLS}$ 대비 분산이 더 작은 estimator가 존재할 수 있음 (이를 테면 $X$에 multicollinearity 문제가 있을 경우 이를 완화하기 위한 Ridge 회귀의 estimator는 biased estimator이지만 분산은 더 작음).</p>
  </li>
  <li>
    <p>A1, A2, A3Rsru, A4가 만족되면, 표본크기가 무한히 많아질 때 OLS estimator는 true $\beta$에 확률수렴함 (확률수렴이란 말은, $X$의 DGP를 고려 시 엄연히 확률변수인 $\hat{\beta}_{OLS}$가, $N$이 무한히 커질 경우 true $\beta$에 매우 가까운 값으로 나올 확률이 100%임을 의미). 
이는 $ \hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon = (X’X/N)^{-1} (X’\epsilon/N) $ 에 A3Rsru 조건 만족 시 $\text{plim} X’\epsilon/N=0$, 그리고 Slutsky theorem으로부터 $\text{plim}(X’X)^{-1}X’\epsilon$을  $\text{plim}(X’X/N)^{-1}=\Sigma_{xx}^{-1}$와 $\text{plim}X’\epsilon/N=0$의 곱으로 나타낼 수 있다는 사실로부터 유도됨.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 asymptotically normal, 즉 표본크기가 크다면 따로 A5 가정을 하지 않아도 근사적으로 정규분포를 따름. $X’\epsilon/N$을 $\epsilon_i$의 표본평균에 $X’$가 곱해진 것으로 해석하면, 중심극한정리에 의해 $\sqrt{N} (X’\epsilon/N - 0) \rightarrow N(0,\sigma^2 \Sigma_{xx})$ 이고 $\sqrt{N} ((X’X/N)^{-1}X’\epsilon/N - 0) = \sqrt{N}(\hat{\beta}_{OLS}-\beta) \rightarrow N(0,\sigma^2 \Sigma_{xx}^{-1})$ 이기 때문.</p>
  </li>
  <li>
    <p>실제로는 오차분산 $\sigma^2$를 정확히 알지 못하므로 표본 기반으로 추정함. A1, A2, A3Rmi, A4 조건이 만족될 경우, $s^2 = \sum_{i=1}^N \hat{\epsilon}^2/(N-k)$ 는 $\sigma^2$의 unbiased and consistent estimator임 ($k$는 intercept를 포함한 parameter의 수). 또한 A5 조건까지 만족될 경우, $(N-k) s^2/\sigma^2 \sim \chi^2(N-k)$ 임.</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 설명변수의 group 기준으로 분해해서 $y = X_1 \beta_1 + X_2 \beta_2 + \epsilon$ 으로 볼 때, $\beta_1$은 $X_2$가 통제된 상태에서 $X_1$만의 변화에 따른 $y$의 변화를 나타내야 함. 그러므로 $\hat{\beta}_1$을 구할 때는, $X_1$로부터 $X_2$의 정보를 projection matrix를 곱해 제거한 $(I - X_2(X_2’X_2)^{-1}X_2’)X_1 = M_2 X_1$ 을 기반으로 회귀해 구해야 함. 즉 $\hat{\beta}_1 = (X_1’M_2 X_1)^{-1} X_1’ M_2 y$ 임 (projection matrix가 idempotent, 즉 $M_2^2 = M_2$임을 이용함).</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 위에서는 $X$의 column을 기준으로 분해했는데, 이와 달리 모델을 row 기준으로 분해할 경우 (이를테면 특정 행 기준으로 old와 new로 구분할 경우), $X’X \hat{\beta} = X’y$ 로부터 아래와 같이 됨. 
$ \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      X_{old} \ 
      X_{new}
  \end{bmatrix}<br />
  \hat{\beta} = 
  \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      y_{old} \ 
      y_{new}
  \end{bmatrix}<br />
  \, \Rightarrow \, [X_{old}’X_{old} + X_{new}’X_{new}] \hat{\beta} = X_{old}’ y_{old} + X_{new}’ y_{new} $
  이를 데이터의 갱신으로 보면 $X_{old}’X_{old}$ 및 $(X_{old}’X_{old})^{-1}$ 가 계산되어 있는 상황이므로, 새로운 데이터 행렬 전체에 대해 새로 계산할 필요 없이 아래 식으로부터 $\hat{\beta}$를 빠르게 계산할 수 있음.
  $ [X_{old}’X_{old} + X_{new}’X_{new}]^{-1} = (X_{old}’X_{old})^{-1} - (I+ X_{new} (X_{old}’X_{old})^{-1} X_{new}’ )^{-1} (X_{old}’X_{old})^{-1} X_{new}’X_{new} (X_{old}’X_{old})^{-1} $</p>
  </li>
</ul>

<h2 id="maximum-likelihood-estimation-in-linear-regression">Maximum likelihood estimation in linear regression</h2>

<ul>
  <li>
    <p>A1부터 A5까지의 가정들이 성립 시, $y_i = x_i’\beta + \epsilon_i$는 $x_i, \beta, \sigma^2$가 주어질 경우 오차항이 iid이며 정규분포를 따른다는 가정으로부터 $y_i | x_i, \beta, \sigma^2 \sim N(x_i’ \beta, \sigma^2)$ 임. 그러므로 $y$ 전체의 likelihood 및 log-likelihood는 아래와 같음.
$ L(y_i | x_i, \beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{(y_i - x_i’\beta)^2}{2 \sigma^2} \right)  = \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) \right)   $
$ \text{log} L(y_i | x_i, \beta, \sigma^2) = - \frac{N}{2} \text{log}(2 \pi) - \frac{N}{2} \text{log}(\sigma^2) - \frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) $
여기서 log-likelihood에 $-(y-X\beta)’(y-X\beta)$ 가 포함되어 있으므로, $\sigma^2$가 주어질 때 log-likelihood를 최대화하는 추정량 $\hat{\beta}_{MLE}$는 $(y-X\beta)’(y-X\beta)$를 최소화하는 추정량, 즉 OLS 추정량과 같음.</p>
  </li>
  <li>
    <p>$\beta$가 주어질 때 log-likelihood를 최대화하는 $\sigma^2$의 추정량은 $\widehat{\sigma^2}_{MLE} = \hat{\epsilon}’ \hat{\epsilon}/N$, 즉 잔차제곱합을 표본 크기로 나눈 값이 됨. 이는 biased estimator임 (OLS에서의 unbiased estimator는 잔차제곱합을 $N-k$로 나눈 값이었음). 그러나 표본크기가 무한히 커지면 $N-k$와 $N$이 비슷해지므로, $\sigma^2$의 maximum likelihood 추정량은 consistent함.</p>
  </li>
  <li>
    <p>MLE에서 parameter를 $\theta$라 할 때 $\theta$의 분산은 log-likelihood의 Hessian의 평균에 마이너스를 붙인 후 역행렬을 취한 $(-\text{E}[H(\theta)])^{-1}$로 주어짐. 이 때 $(-\text{E}[H(\theta)])$ 를 information matrix $I(\theta)$라 부르기도 하며, $\text{Var}[\theta] = I(\theta)^{-1}$ 임. 
이에 따라 계산하면 $\text{Var}[\hat{\beta}_{MLE}] = \sigma^2(X’X)^{-1}$을 얻고 (OLS estimator의 공분산행렬과 같음), $\text{Var}[\widehat{\sigma^2}_{MLE}] = 2\sigma^4/N$을 얻음.</p>
  </li>
  <li>
    <p>MLE로 추정한 추정량은 consistent하며, asymptotic normal이고, Consistent Uniformly Asymptotically Normal (CUAN) 성질을 갖는 estimator들 중 가장 효율적인 추정량임 (MLE 추정량의 공분산행렬은 CUAN estimator들의 Cramer-Rao lower bound, 즉 unbiased estimator가 가지는 분산의 하한임). 그러므로 오차항의 분포에 확신이 있다면, MLE를 쓰는 것이 가장 좋음.</p>
  </li>
</ul>]]></content><author><name>Jeonghun Song</name></author><category term="Theory" /><summary type="html"><![CDATA[Ordinary least squares]]></summary></entry><entry><title type="html">회귀분석 관련 정리 1</title><link href="http://localhost:4000/2023/04/regressionone.html" rel="alternate" type="text/html" title="회귀분석 관련 정리 1" /><published>2023-04-09T00:00:00+09:00</published><updated>2023-04-09T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/regressionone</id><content type="html" xml:base="http://localhost:4000/2023/04/regressionone.html"><![CDATA[<h2 id="ordinary-least-squares">Ordinary least squares</h2>

<ul>
  <li>
    <p>선형회귀모델 $y=X \beta + \epsilon$ 에는 아래의 다섯가지 가정이 들어감.</p>

    <p>A1:  $X$는 full rank임 (즉 특정 설명변수를 다른 설명변수들의 선형결합으로 완전하게 나타낼 수 없으며, $X’X$의 역행렬이 존재). 이 조건이 깨지면 estimator를 구할 수 없음.</p>

    <p>A2: 설명변수와 반응변수 간에 선형관계가 성립하고, 오차항의 편향이 없음 (즉 $\text{E}[\epsilon] = 0$).</p>

    <p>A3: 설명변수 $X$와 오차항 $\epsilon$ 간에는 상관관계가 없음. $X$가 fixed된 값들이거나 (A3F) random이더라도 $\epsilon$과 완전히 독립인 경우 (A3Rfi), 또는 random인 $X$가 전부 주어졌을 때 오차항의 조건부평균 $\text{E}[ \epsilon_{t} | X] = 0$ 이면 (A3Rmi), estimator는 unbiased이면서 consistent함. 만약 $x_i$가 적어도 같은 row의 $\epsilon_{i}$와는 상관이 없다면 (A3Rsru), estimator는 biased일 수는 있으나 consistent함 (이를 테면 시계열에서 시점 $t$의 설명변수는, 과거의 오차항과는 상관이 있으나 현재 및 미래의 오차항과는 상관이 없음.</p>

    <p>A4: 오차항은 서로 독립이고 같은 분포를 따름, 즉 상관되어 있지 않고 일정한 분산을 가짐 (즉 $\text{E}[\epsilon \epsilon’ | X] = \sigma^2 I_N$). 이 조건이 깨지면 Generalized least square가 필요.</p>

    <p>A5: 오차항이 정규분포를 따름. A5가 만족될 경우 MLE estimator가 OLS estimator와 같으므로 OLS가 효율적임. 꼭 정규분포는 아니라도 오차항의 분포를 알아야 MLE를 쓸 수 있음.</p>
  </li>
  <li>
    <p>OLS에서 목적함수는 반응변수의 실제값과 추정값(설명변수 및 선형모델로 계산된) 간 차이의 제곱합인 $\sum_{i=1}^N [(y_i - x_i’ \beta)^2 ] = (y-X\beta)’(y-X\beta)$ 이며, 이를 최소화하는 해는 $\hat{\beta}_{OLS}=(X’X)^{-1}X’y$ 임.</p>
  </li>
  <li>
    <p>Vector space 관점에서 보면, $X\hat{\beta}_{OLS}$는 $X$의 열공간의 원소이므로 $X\hat{\beta}_{OLS} = X(X’X)^{-1}X’y = \hat{y}$ 는 $y$를 $X$의 열공간에 정사영한 벡터, 즉 $X$의 열공간에 속하는 벡터 중 $y$에 가장 가까운 벡터임. 이로부터 $y$를 $X$의 열공간 내의 벡터로 변환하는 사상에 대응되는 행렬 $X(X’X)^{-1}X’$를 hat matrix라 하며, $y$의 정보 중 $X$로 설명되는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>A3Rsru 조건 $\text{E}[x_i \epsilon_i] = 0$ 으로부터 sample moment condition $\sum_{i=1}^N x_i (y_i - x_i’ \beta)/N = 0$ 을 세워 계산한 method of moments estimator $\hat{\beta}_{MME}$ 는 $\hat{\beta}_{OLS}$와 같음.</p>
  </li>
  <li>
    <p>$y$와 $\hat{y}$ 간 차이는 잔차(residual, $\hat{\epsilon}$)임. 잔차벡터를 식으로 나타내면 $\epsilon = y - \hat{y} = (I-X(X’X)^{-1}X’)y$ 임. 행렬 $I-X(X’X)^{-1}X’$를 projection matrix라 하며, $y$의 정보 중 $X$로 설명되지 않는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>OLS에서의 잔차는 실제 오차와 다른 개념임. 실제로는 오차가 A4GM 가정을 만족하지 않더라도 OLS 잔차는 A4GM이 만족됨을 가정하고 계산된 값임. 그러므로 반드시 잔차 plot을 그려서 이분산이나 오차의 자기상관 등이 의심되는지를 확인해야 함. 또한 오차가 설명변수와 상관이 있든 없든, OLS 계산 결과인 잔차는 항상 $X$와 직교함 ($X \hat{\epsilon} = 0$). 그러므로 A3가정의 만족 여부는 잔차만으로는 알 수 없으며 endogeneity 관련 방법론이 필요함.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi가 만족되면, $\text{E}[\hat{\beta}_{OLS} | X] = \beta$, 즉 OLS estimator는 unbiased임 ($y=X\beta+\epsilon$으로부터 유도). 또한 OLS estimator의 공분산행렬을 계산하면 $\text{Var}[\beta|X] = \sigma^2 (X’X)^{-1}$ 임 ($\hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon$ 으로부터 유도).</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 unbiased linear estimator ($\text{E}[\tilde{\beta}] = \beta$ 즉 unbiased 이면서, 어떤 full rank 행렬 A ($AX=I$) 에 대해 $\tilde{\beta} = Ay$ 즉 $y$에 대해 linear인 임의의 estimator) 들 중 분산이 가장 작은 (즉 가장 효율적인) estimator임. 이를 Gauss-Markov theorem이라 함.</p>
  </li>
  <li>
    <p>Gauss-Markov theorem에서는 A5 (오차항의 정규분포 가정) 가 필요하지 않았음에 주의. 또한 biased되어 있거나 nonlinear인 estimator 중에서는 $\hat{\beta}_{OLS}$ 대비 분산이 더 작은 estimator가 존재할 수 있음 (이를 테면 $X$에 multicollinearity 문제가 있을 경우 이를 완화하기 위한 Ridge 회귀의 estimator는 biased estimator이지만 분산은 더 작음).</p>
  </li>
  <li>
    <p>A1, A2, A3Rsru, A4가 만족되면, 표본크기가 무한히 많아질 때 OLS estimator는 true $\beta$에 확률수렴함 (확률수렴이란 말은, $X$의 DGP를 고려 시 엄연히 확률변수인 $\hat{\beta}_{OLS}$가, $N$이 무한히 커질 경우 true $\beta$에 매우 가까운 값으로 나올 확률이 100%임을 의미). 
이는 $ \hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon = (X’X/N)^{-1} (X’\epsilon/N) $ 에 A3Rsru 조건 만족 시 $\text{plim} X’\epsilon/N=0$, 그리고 Slutsky theorem으로부터 $\text{plim}(X’X)^{-1}X’\epsilon$을  $\text{plim}(X’X/N)^{-1}=\Sigma_{xx}^{-1}$와 $\text{plim}X’\epsilon/N=0$의 곱으로 나타낼 수 있다는 사실로부터 유도됨.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 asymptotically normal, 즉 표본크기가 크다면 따로 A5 가정을 하지 않아도 근사적으로 정규분포를 따름. $X’\epsilon/N$을 $\epsilon_i$의 표본평균에 $X’$가 곱해진 것으로 해석하면, 중심극한정리에 의해 $\sqrt{N} (X’\epsilon/N - 0) \rightarrow N(0,\sigma^2 \Sigma_{xx})$ 이고 $\sqrt{N} ((X’X/N)^{-1}X’\epsilon/N - 0) = \sqrt{N}(\hat{\beta}_{OLS}-\beta) \rightarrow N(0,\sigma^2 \Sigma_{xx}^{-1})$ 이기 때문.</p>
  </li>
  <li>
    <p>실제로는 오차분산 $\sigma^2$를 정확히 알지 못하므로 표본 기반으로 추정함. A1, A2, A3Rmi, A4 조건이 만족될 경우, $s^2 = \sum_{i=1}^N \hat{\epsilon}^2/(N-k)$ 는 $\sigma^2$의 unbiased and consistent estimator임 ($k$는 intercept를 포함한 parameter의 수). 또한 A5 조건까지 만족될 경우, $(N-k) s^2/\sigma^2 \sim \chi^2(N-k)$ 임.</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 설명변수의 group 기준으로 분해해서 $y = X_1 \beta_1 + X_2 \beta_2 + \epsilon$ 으로 볼 때, $\beta_1$은 $X_2$가 통제된 상태에서 $X_1$만의 변화에 따른 $y$의 변화를 나타내야 함. 그러므로 $\hat{\beta}_1$을 구할 때는, $X_1$로부터 $X_2$의 정보를 projection matrix를 곱해 제거한 $(I - X_2(X_2’X_2)^{-1}X_2’)X_1 = M_2 X_1$ 을 기반으로 회귀해 구해야 함. 즉 $\hat{\beta}_1 = (X_1’M_2 X_1)^{-1} X_1’ M_2 y$ 임 (projection matrix가 idempotent, 즉 $M_2^2 = M_2$임을 이용함).</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 위에서는 $X$의 column을 기준으로 분해했는데, 이와 달리 모델을 row 기준으로 분해할 경우 (이를테면 특정 행 기준으로 old와 new로 구분할 경우), $X’X \hat{\beta} = X’y$ 로부터 아래와 같이 됨. 
$ \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      X_{old} \ 
      X_{new}
  \end{bmatrix}<br />
  \hat{\beta} = 
  \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      y_{old} \ 
      y_{new}
  \end{bmatrix}<br />
  \, \Rightarrow \, [X_{old}’X_{old} + X_{new}’X_{new}] \hat{\beta} = X_{old}’ y_{old} + X_{new}’ y_{new} $
  이를 데이터의 갱신으로 보면 $X_{old}’X_{old}$ 및 $(X_{old}’X_{old})^{-1}$ 가 계산되어 있는 상황이므로, 새로운 데이터 행렬 전체에 대해 새로 계산할 필요 없이 아래 식으로부터 $\hat{\beta}$를 빠르게 계산할 수 있음.
  $ [X_{old}’X_{old} + X_{new}’X_{new}]^{-1} = (X_{old}’X_{old})^{-1} - (I+ X_{new} (X_{old}’X_{old})^{-1} X_{new}’ )^{-1} (X_{old}’X_{old})^{-1} X_{new}’X_{new} (X_{old}’X_{old})^{-1} $</p>
  </li>
</ul>

<h2 id="maximum-likelihood-estimation-in-linear-regression">Maximum likelihood estimation in linear regression</h2>

<ul>
  <li>
    <p>A1부터 A5까지의 가정들이 성립 시, $y_i = x_i’\beta + \epsilon_i$는 $x_i, \beta, \sigma^2$가 주어질 경우 오차항이 iid이며 정규분포를 따른다는 가정으로부터 $y_i | x_i, \beta, \sigma^2 \sim N(x_i’ \beta, \sigma^2)$ 임. 그러므로 $y$ 전체의 likelihood 및 log-likelihood는 아래와 같음.
$ L(y_i | x_i, \beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{(y_i - x_i’\beta)^2}{2 \sigma^2} \right)  = \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) \right)   $
$ \text{log} L(y_i | x_i, \beta, \sigma^2) = - \frac{N}{2} \text{log}(2 \pi) - \frac{N}{2} \text{log}(\sigma^2) - \frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) $
여기서 log-likelihood에 $-(y-X\beta)’(y-X\beta)$ 가 포함되어 있으므로, $\sigma^2$가 주어질 때 log-likelihood를 최대화하는 추정량 $\hat{\beta}_{MLE}$는 $(y-X\beta)’(y-X\beta)$를 최소화하는 추정량, 즉 OLS 추정량과 같음.</p>
  </li>
  <li>
    <p>$\beta$가 주어질 때 log-likelihood를 최대화하는 $\sigma^2$의 추정량은 $\widehat{\sigma^2}_{MLE} = \hat{\epsilon}’ \hat{\epsilon}/N$, 즉 잔차제곱합을 표본 크기로 나눈 값이 됨. 이는 biased estimator임 (OLS에서의 unbiased estimator는 잔차제곱합을 $N-k$로 나눈 값이었음). 그러나 표본크기가 무한히 커지면 $N-k$와 $N$이 비슷해지므로, $\sigma^2$의 maximum likelihood 추정량은 consistent함.</p>
  </li>
  <li>
    <p>MLE에서 parameter를 $\theta$라 할 때 $\theta$의 분산은 log-likelihood의 Hessian의 평균에 마이너스를 붙인 후 역행렬을 취한 $(-\text{E}[H(\theta)])^{-1}$로 주어짐. 이 때 $(-\text{E}[H(\theta)])$ 를 information matrix $I(\theta)$라 부르기도 하며, $\text{Var}[\theta] = I(\theta)^{-1}$ 임. 
이에 따라 계산하면 $\text{Var}[\hat{\beta}_{MLE}] = \sigma^2(X’X)^{-1}$을 얻고 (OLS estimator의 공분산행렬과 같음), $\text{Var}[\widehat{\sigma^2}_{MLE}] = 2\sigma^4/N$을 얻음.</p>
  </li>
  <li>
    <p>MLE로 추정한 추정량은 consistent하며, asymptotic normal이고, Consistent Uniformly Asymptotically Normal (CUAN) 성질을 갖는 estimator들 중 가장 효율적인 추정량임 (MLE 추정량의 공분산행렬은 CUAN estimator들의 Cramer-Rao lower bound, 즉 unbiased estimator가 가지는 분산의 하한임). 그러므로 오차항의 분포에 확신이 있다면, MLE를 쓰는 것이 가장 좋음.</p>
  </li>
</ul>]]></content><author><name>Jeonghun Song</name></author><category term="Theory" /><summary type="html"><![CDATA[Ordinary least squares]]></summary></entry><entry><title type="html">First test</title><link href="http://localhost:4000/2023/04/first.html" rel="alternate" type="text/html" title="First test" /><published>2023-04-04T00:00:00+09:00</published><updated>2023-04-04T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/first</id><content type="html" xml:base="http://localhost:4000/2023/04/first.html"><![CDATA[<h1 id="learning-github-page">Learning github page</h1>

<p>First time for github page</p>]]></content><author><name>Jeonghun Song</name></author><category term="etc" /><summary type="html"><![CDATA[Learning github page]]></summary></entry><entry><title type="html">Web scrapper example</title><link href="http://localhost:4000/2023/04/scrapper.html" rel="alternate" type="text/html" title="Web scrapper example" /><published>2023-04-04T00:00:00+09:00</published><updated>2023-04-04T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/scrapper</id><content type="html" xml:base="http://localhost:4000/2023/04/scrapper.html"><![CDATA[<h2 id="소개">소개</h2>
<p>아래는 PubMed 사이트에서 원하는 논문 정보들을 수집하는 Web scrapping 코드입니다. 친구한테 만들어줬었습니다.</p>

<h2 id="코드">코드</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="n">urllib.parse</span> <span class="kn">import</span> <span class="n">quote</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># Each tuple in list variable "searchwordlist" contains searchword as the first element,
# and desired number of search result pages as the second element 
</span><span class="n">searchwordlist</span> <span class="o">=</span> <span class="p">[</span>
<span class="p">(</span><span class="s">"""
Venoms[mesh] AND pathology[affiliation]
"""</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">(</span><span class="s">"""
Turtle[mesh] AND pathology[affiliation]
"""</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="p">]</span> 



<span class="c1">### Below is the code for scrapping
</span>
<span class="k">def</span> <span class="nf">get_html</span><span class="p">(</span><span class="n">url</span><span class="p">):</span> 
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> 
    <span class="c1"># retry if failed to get html of the target webpage
</span>    <span class="k">except</span> <span class="nb">Exception</span><span class="p">:</span>  
        <span class="nf">print</span><span class="p">(</span><span class="s">"Retrying"</span><span class="p">)</span>
        <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">get_html</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="n">starttime</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">for </span><span class="p">(</span><span class="n">searchword</span><span class="p">,</span><span class="n">numpages</span><span class="p">)</span> <span class="ow">in</span> <span class="n">searchwordlist</span><span class="p">:</span>

    <span class="n">urls</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Initialize the lists for storing scraped data
</span>    <span class="n">papertitle</span><span class="o">=</span> <span class="p">[]</span>
    <span class="n">date</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">affiliations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">authors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">journals</span> <span class="o">=</span> <span class="p">[]</span>   

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">numpages</span><span class="p">):</span> <span class="c1"># for each searchresult page
</span>    
        <span class="n">url_searchpage</span> <span class="o">=</span> <span class="s">"https://pubmed.ncbi.nlm.nih.gov/?term="</span> <span class="o">+</span> <span class="nf">quote</span><span class="p">(</span><span class="n">searchword</span><span class="p">)</span> <span class="o">+</span> <span class="s">"&amp;page={}"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># make URL of searchresult page
</span>        <span class="n">getdata</span> <span class="o">=</span> <span class="nf">get_html</span><span class="p">(</span><span class="n">url_searchpage</span><span class="p">)</span> <span class="c1"># get HTML of the page
</span>        <span class="n">getdata</span><span class="p">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="s">"UTF-8"</span>
        <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">getdata</span><span class="p">,</span> <span class="s">'html.parser'</span><span class="p">)</span> <span class="c1"># parse the HTML 
</span>        <span class="n">titles</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="s">"a"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"docsum-title"</span><span class="p">})</span> 
             
        <span class="k">for</span> <span class="n">title</span> <span class="ow">in</span> <span class="n">titles</span><span class="p">:</span> <span class="c1"># for each paper page
</span>            <span class="n">url_paperpage</span> <span class="o">=</span> <span class="s">"https://pubmed.ncbi.nlm.nih.gov/"</span> <span class="o">+</span> <span class="n">title</span><span class="p">[</span><span class="s">"data-article-id"</span><span class="p">]</span> <span class="c1"># make URL of paper page (title["data-article-id"] is an ID of the paper in PubMed)
</span>            <span class="n">urls</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">url_paperpage</span><span class="p">)</span> <span class="c1"># store URL to the list
</span>            <span class="n">getdata</span> <span class="o">=</span> <span class="nf">get_html</span><span class="p">(</span><span class="n">url_paperpage</span><span class="p">)</span> <span class="c1"># get HTML of the page
</span>            <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">getdata</span><span class="p">,</span> <span class="s">'html.parser'</span><span class="p">)</span> <span class="c1"># parse the HTML
</span>            
            <span class="n">papertitle</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"h1"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"heading-title"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">lstrip</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">())</span> <span class="c1"># store title to the list, after processing the data using functions such as replace, lstrip, rstrip
</span>            
            <span class="k">if</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"span"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"cit"</span><span class="p">})</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> 
                <span class="n">date</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s">"No date"</span><span class="p">)</span> <span class="c1"># if there is no information of date of publication, store a default message
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">date</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"span"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"cit"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="s">";"</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># store date of publication to the list, after processing the data using functions such as replace, lstrip, rstrip
</span>                
            <span class="k">if</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"authors"</span><span class="p">})</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>   
                <span class="n">authors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s">"No author"</span><span class="p">)</span> <span class="c1"># if there is no information of authors, store a default message
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">authors</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"authors"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="s">"                "</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="s">"              "</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="s">"Authors"</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="s">"Author"</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="nf">lstrip</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">())</span> <span class="c1"># store authors to the list, after processing the data using functions such as replace, lstrip, rstrip
</span>                
            <span class="k">if</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"affiliations"</span><span class="p">})</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">affiliations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s">"No affiliation"</span><span class="p">)</span> <span class="c1"># if there is no information of affiliation, store a default message
</span>            <span class="k">else</span><span class="p">:</span> 
                <span class="n">affiliations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"affiliations"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="s">"Affiliations"</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="s">"Affiliation"</span><span class="p">,</span><span class="s">""</span><span class="p">).</span><span class="nf">lstrip</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">())</span>  <span class="c1"># store affiliation to the list, after processing the data using functions such as replace, lstrip, rstrip
</span>            
            <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"abstract"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="s">"Keywords:"</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">keywords</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s">"No keywords"</span><span class="p">)</span> <span class="c1"># if there is no information of keywords, store a default message
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">keywords</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"abstract"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="s">"Keywords:"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">lstrip</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">())</span> <span class="c1"># store keywords to the list, after processing the data using functions such as replace, lstrip, rstrip
</span>                
            <span class="k">if</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"span"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"journal"</span><span class="p">})</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> 
                <span class="n">journals</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s">"No journal"</span><span class="p">)</span> <span class="c1"># if there is no information of journal, store a default message
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">journals</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">soup</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="s">"span"</span><span class="p">,{</span><span class="s">"class"</span><span class="p">:</span><span class="s">"journal"</span><span class="p">}).</span><span class="n">text</span><span class="p">.</span><span class="nf">lstrip</span><span class="p">().</span><span class="nf">rstrip</span><span class="p">())</span> <span class="c1"># store journal to the list, after processing the data using functions such as replace, lstrip, rstrip
</span>                
        <span class="n">checktime</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">"{} of {} pages scraped. {} seconds elapsed"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">numpages</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">checktime</span><span class="o">-</span><span class="n">starttime</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span> <span class="c1"># display a message informing progress
</span>
    <span class="n">df_papers</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span> <span class="c1"># combine the lists containing data as a dataframe
</span>        <span class="p">{</span>
            <span class="s">"Title"</span><span class="p">:</span> <span class="n">papertitle</span><span class="p">,</span>
            <span class="s">"Authors"</span><span class="p">:</span> <span class="n">authors</span><span class="p">,</span>
            <span class="s">"Affiliations"</span><span class="p">:</span> <span class="n">affiliations</span><span class="p">,</span>
            <span class="s">"Keywords"</span><span class="p">:</span> <span class="n">keywords</span><span class="p">,</span>
            <span class="s">"Date"</span><span class="p">:</span> <span class="n">date</span><span class="p">,</span>
            <span class="s">"Journal"</span><span class="p">:</span> <span class="n">journals</span><span class="p">,</span>
            <span class="s">"URL"</span><span class="p">:</span> <span class="n">urls</span><span class="p">,</span>
            <span class="s">"Searchword:{}"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">searchword</span><span class="p">):</span> <span class="bp">None</span>
        <span class="p">}</span>
    <span class="p">)</span>
    
    <span class="n">df_papers</span><span class="p">.</span><span class="nf">to_excel</span><span class="p">(</span><span class="s">'results_{}.xlsx'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">replace</span><span class="p">(</span><span class="n">microsecond</span><span class="o">=</span><span class="mi">0</span><span class="p">)).</span><span class="nf">replace</span><span class="p">(</span><span class="s">":"</span><span class="p">,</span><span class="s">"-"</span><span class="p">)))</span> <span class="c1"># export the dataframe as an excel file    
</span>    <span class="nf">print</span><span class="p">(</span><span class="s">"Excel file exported."</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Jeonghun Song</name></author><category term="Code" /><category term="python" /><category term="BeautifulSoup" /><summary type="html"><![CDATA[소개 아래는 PubMed 사이트에서 원하는 논문 정보들을 수집하는 Web scrapping 코드입니다. 친구한테 만들어줬었습니다.]]></summary></entry><entry><title type="html">Example of MD file</title><link href="http://localhost:4000/2023/04/termproject.html" rel="alternate" type="text/html" title="Example of MD file" /><published>2023-04-04T00:00:00+09:00</published><updated>2023-04-04T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/termproject</id><content type="html" xml:base="http://localhost:4000/2023/04/termproject.html"><![CDATA[<h1 id="term-project-for-com506-data-management">Term project for COM506-Data Management</h1>
<h2 id="an-automated-data-pipeline-for-scraping-book-information-from-an-online-bookseller-and-storing-it-to-sql-nosql-databases">An automated data pipeline for scraping book information from an online bookseller and storing it to SQL/ NoSQL databases</h2>
<p>Jeonghun Song (20212210010), submitted in 6th September 2022</p>

<p><img src="https://d1sslqz50ui5dv.cloudfront.net/wp-content/uploads/2021/07/26094151/siai_logo_prev.png" width="600px" /></p>

<h2 id="project-charter">Project Charter</h2>

<h3 id="problem-statement">Problem statement</h3>
<ul>
  <li>Most of online booksellers do not provide a convenient way to get bulk data of books such as open API or a directy way to send queries.</li>
  <li>A tool collecting, processing and storing data of a big online bookseller in an automated way is required for sucht tasks. <br />
Also, the tool should be able to handle not only basic attributes of books but also description and reviews on the books.</li>
</ul>

<h3 id="business-case">Business case</h3>
<ul>
  <li>Deciding books with higher priority in a specific field with multiple searchwords</li>
  <li>Investigation on authors or publishers who are eminent in a specific field</li>
  <li>Estimation of popularity trend in a specific field by investigation on book publishing records in a temporal view</li>
</ul>

<h3 id="goal-statement">Goal statement</h3>
<ul>
  <li>Deliver a automated data pipeline which collects information of desired numbers of books (including reviews) from a bookseller website for given searchwords, processes the data, and stores in database.</li>
  <li>Deliver a query script for generating desired subsets of the stored book data</li>
</ul>

<h3 id="timeline">Timeline</h3>
<ul>
  <li>Deciding requirements (August 20)</li>
  <li>Planing (Entity-Relationship Diagram, JSON Schema) (August 21)</li>
  <li>Writing code of collect stage module and testing it (August 27)</li>
  <li>Writing code of process stage module and testing it (August 28)</li>
  <li>Writing code of store stage module and testing it (August 28)</li>
  <li>Writing code of pipeline which automates collect - process - store stages and testing it (September 3)</li>
  <li>Writing code of query stage module and testing it (September 3)</li>
  <li>Refactoring codes and adding comments (September 4)</li>
  <li>Writing README.md and IMRaD document (September 6)</li>
</ul>

<h3 id="scope">Scope</h3>
<ul>
  <li>Target bookseller: https://www.bookdepository.com/</li>
  <li>Target website for external reviews on the books: https://www.goodreads.com/</li>
  <li>Collects book data for each searchword</li>
  <li>Unlimited number of searchwords and number of books per searchword</li>
  <li>Up to top 30 reviews per book</li>
  <li>Collected product details stored to SQLite DB as tables, and collected reviews stored to Elasticsearch DB as an index</li>
  <li>Functions out of scope: Data analysis and data visualization, collecting pictures, collecting side information such as ‘People who bought this also bought’ or ‘Bestsellers in the field’ for each book</li>
</ul>

<h3 id="member">Member</h3>
<ul>
  <li>Project Manager &amp; Developer: Jeonghun Song, MSc in Data Science, F2021, Swiss Inistitue of Artificial Intelligence</li>
</ul>

<h2 id="how-to-run">How to run</h2>

<h3 id="prerequisite">Prerequisite</h3>
<ul>
  <li>Installation of packages in requirements.txt<br />
(Note that the version of pandas should be 1.4.1 or later)</li>
  <li>Installation of SQLite (Precompiled Binaries for Windows): https://www.sqlite.org/download.html</li>
  <li>Installation of Docker (follow instructions in Chapter 4 of OPT101 - Issues in Computer Programming)</li>
  <li>Installation of Elasticsearch &amp; Kibana with Docker image given in Lecture 6 of COM506 - Data Management</li>
  <li>Running the docker container containing Elasticsearch &amp; Kibana</li>
</ul>

<h3 id="data-pipeline-web-scraping-processing-and-storing-data">Data pipeline (Web scraping, processing and storing data)</h3>
<ul>
  <li>Open maincode.py at the root folder.</li>
  <li>Make tuples of size 2 each. The first element of the tuple is the searchword, and the second element is the desired number of books to scrap for the searchword.</li>
  <li>Put the tuples into variable “list_search”.</li>
  <li>Run the code.<br />
You will see messages “Collect stage starts.”, “x of y books scraped.” in the terminal.<br />
The data pipline modules in “src” directory automatically collects the book data, processes them as CSV files and JSON file, and stores them into SQLite DB and Elasticsearch.</li>
  <li>When terminated, the CSV and JSON files are stored in “data” folder.</li>
  <li>The SQLite DB can be directly accessed by opening db_books.db in “data” folder.</li>
  <li>The Elasticsearch DB can be accessed by entering “localhost:5601” in web browser when the docker container contaning Elasticsearch and Kibana is running.</li>
  <li>If tables and index corresponding to the same searchword already exists in the DBs, these are overwritten by the new tables and index created by the current operation.</li>
  <li>Duplicates (ex&gt; hardcover and paperback of a same book) and unavailable books (no prices) are dropped from the initially gathered data in process stage.<br />
Thus, the processed Bookinfo table and index may contain books less than the desired number which has been put into the second element of the tuple in list_search.</li>
</ul>

<h3 id="outputs-of-the-data-pipeline">Outputs of the data pipeline</h3>
<ul>
  <li>Output per searchword: 3 tables (exported as CSV files and stored in SQLite DB),<br />
1 set of dictionaries (exported as JSON file and and stored in lasticsearch as an index)</li>
  <li>Information collected to the first CSV file (Bookinfo):<br />
ISBN-13 (book identifier), Title, Author, Publisher, Rating (out of 5), Number of ratings, Publication Date, Price (in KRW), Number of pages</li>
  <li>Information collected to the second CSV file (Author):<br />
Author, Number of books written by the author, bestseller of the author (with the largest number of ratings), number of ratings on the bestseller</li>
  <li>Information collected to the third CSV file (Publisher):<br />
Author, Number of books published by the publisher, bestseller of the publisher (with the largest number of ratings), number of ratings on the bestseller</li>
  <li>Information collected to the JSON file:<br />
Title, URL of the webpage, Description of the books, Reviews on the books (up to 30 reviews per book)</li>
</ul>

<h3 id="separated-codes-for-sending-query-to-the-dbs">Separated codes for sending query to the DBs</h3>

<ul>
  <li>Open query_SQLite.py if you want to get data stored in SQLite DB.</li>
  <li>Open query_Elasticsearch.py if you want to get data stored in Elasticsearch.</li>
  <li>Put the corresponding searchword into variable “searchword”.</li>
  <li>Put your query statement into variable “your_query”.</li>
  <li>Put the desired file name into variable “filename”. You don’t have to put extension (.csv, .json) into “filename”.</li>
  <li>Run the code.</li>
  <li>When terminated, results for the query are stored in “query_results” folder under “data” folder, as CSV (SQLite) or JSON (Elasticsearch) file.</li>
</ul>

<p>#
#</p>
<h2 id="project-directory-structure">Project Directory Structure</h2>
<pre><code class="language-cmd">.
├── README.md
├── data
│   ├── query_results
│   │    ├── queryresult_econometrics.csv
│   │    └── queryresult_linear_algebra.json
│   ├── db_books.db 
│   ├── reviews_econometrics.json
│   ├── reviews_linear algebra.json 
│   ├── table_Author_econometrics.csv
│   ├── table_Author_linear algebra.csv
│   ├── table_Book_econometrics.csv
│   ├── table_Book_linear algebra.csv
│   ├── table_Publisher_econometrics.csv
│   └── table_Publisher_linear algebra.csv
├── documents
│   ├── IMRaD document.pdf
│   ├── erd_for_SQL.png
│   ├── erd_for_SQL.puml
│   └── JSON_SCHEMA.md
├── maincode.py
├── query_Elasticsearch.py
├── query_SQLite.py 
├── requirements.txt
└── src
    ├── getbookdata.py
    ├── module_collect.py
    ├── module_process.py
    ├── module_store.py
    ├── settings.py
    └── validation_jsonschema.py

</code></pre>]]></content><author><name>Jeonghun Song</name></author><category term="Projects" /><summary type="html"><![CDATA[Term project for COM506-Data Management An automated data pipeline for scraping book information from an online bookseller and storing it to SQL/ NoSQL databases Jeonghun Song (20212210010), submitted in 6th September 2022]]></summary></entry><entry><title type="html">회귀분석 관련 정리</title><link href="http://localhost:4000/2023/04/regression.html" rel="alternate" type="text/html" title="회귀분석 관련 정리" /><published>2023-04-04T00:00:00+09:00</published><updated>2023-04-04T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/regression</id><content type="html" xml:base="http://localhost:4000/2023/04/regression.html"><![CDATA[<h2 id="ordinary-least-squares">Ordinary least squares</h2>

<ul>
  <li>
    <p>선형회귀모델 $y=X \beta + \epsilon$ 에는 아래의 다섯가지 가정이 들어감.</p>

    <p>A1:  $X$는 full rank임 (즉 특정 설명변수를 다른 설명변수들의 선형결합으로 완전하게 나타낼 수 없으며, $X’X$의 역행렬이 존재). 이 조건이 깨지면 estimator를 구할 수 없음.</p>

    <p>A2: 설명변수와 반응변수 간에 선형관계가 성립하고, 오차항의 편향이 없음 (즉 $\text{E}[\epsilon] = 0$).</p>

    <p>A3: 설명변수 $X$와 오차항 $\epsilon$ 간에는 상관관계가 없음. $X$가 fixed된 값들이거나 (A3F) random이더라도 $\epsilon$과 완전히 독립인 경우 (A3Rfi), 또는 random인 $X$가 전부 주어졌을 때 오차항의 조건부평균 $\text{E}[ \epsilon_{t} | X] = 0$ 이면 (A3Rmi), estimator는 unbiased이면서 consistent함. 만약 $x_i$가 적어도 같은 row의 $\epsilon_{i}$와는 상관이 없다면 (A3Rsru), estimator는 biased일 수는 있으나 consistent함 (이를 테면 시계열에서 시점 $t$의 설명변수는, 과거의 오차항과는 상관이 있으나 현재 및 미래의 오차항과는 상관이 없음.</p>

    <p>A4: 오차항은 서로 독립이고 같은 분포를 따름, 즉 상관되어 있지 않고 일정한 분산을 가짐 (즉 $\text{E}[\epsilon \epsilon’ | X] = \sigma^2 I_N$). 이 조건이 깨지면 Generalized least square가 필요.</p>

    <p>A5: 오차항이 정규분포를 따름. A5가 만족될 경우 MLE estimator가 OLS estimator와 같으므로 OLS가 효율적임. 꼭 정규분포는 아니라도 오차항의 분포를 알아야 MLE를 쓸 수 있음.</p>
  </li>
  <li>
    <p>OLS에서 목적함수는 반응변수의 실제값과 추정값(설명변수 및 선형모델로 계산된) 간 차이의 제곱합인 $\sum_{i=1}^N [(y_i - x_i’ \beta)^2 ] = (y-X\beta)’(y-X\beta)$ 이며, 이를 최소화하는 해는 $\hat{\beta}_{OLS}=(X’X)^{-1}X’y$ 임.</p>
  </li>
  <li>
    <p>Vector space 관점에서 보면, $X\hat{\beta}_{OLS}$는 $X$의 열공간의 원소이므로 $X\hat{\beta}_{OLS} = X(X’X)^{-1}X’y = \hat{y}$ 는 $y$를 $X$의 열공간에 정사영한 벡터, 즉 $X$의 열공간에 속하는 벡터 중 $y$에 가장 가까운 벡터임. 이로부터 $y$를 $X$의 열공간 내의 벡터로 변환하는 사상에 대응되는 행렬 $X(X’X)^{-1}X’$를 hat matrix라 하며, $y$의 정보 중 $X$로 설명되는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>A3Rsru 조건 $\text{E}[x_i \epsilon_i] = 0$ 으로부터 sample moment condition $\sum_{i=1}^N x_i (y_i - x_i’ \beta)/N = 0$ 을 세워 계산한 method of moments estimator $\hat{\beta}_{MME}$ 는 $\hat{\beta}_{OLS}$와 같음.</p>
  </li>
  <li>
    <p>$y$와 $\hat{y}$ 간 차이는 잔차(residual, $\hat{\epsilon}$)임. 잔차벡터를 식으로 나타내면 $\epsilon = y - \hat{y} = (I-X(X’X)^{-1}X’)y$ 임. 행렬 $I-X(X’X)^{-1}X’$를 projection matrix라 하며, $y$의 정보 중 $X$로 설명되지 않는 정보만 남긴 결과로 볼 수 있음.</p>
  </li>
  <li>
    <p>OLS에서의 잔차는 실제 오차와 다른 개념임. 실제로는 오차가 A4GM 가정을 만족하지 않더라도 OLS 잔차는 A4GM이 만족됨을 가정하고 계산된 값임. 그러므로 반드시 잔차 plot을 그려서 이분산이나 오차의 자기상관 등이 의심되는지를 확인해야 함. 또한 오차가 설명변수와 상관이 있든 없든, OLS 계산 결과인 잔차는 항상 $X$와 직교함 ($X \hat{\epsilon} = 0$). 그러므로 A3가정의 만족 여부는 잔차만으로는 알 수 없으며 endogeneity 관련 방법론이 필요함.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi가 만족되면, $\text{E}[\hat{\beta}_{OLS} | X] = \beta$, 즉 OLS estimator는 unbiased임 ($y=X\beta+\epsilon$으로부터 유도). 또한 OLS estimator의 공분산행렬을 계산하면 $\text{Var}[\beta|X] = \sigma^2 (X’X)^{-1}$ 임 ($\hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon$ 으로부터 유도).</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 unbiased linear estimator ($\text{E}[\tilde{\beta}] = \beta$ 즉 unbiased 이면서, 어떤 full rank 행렬 A ($AX=I$) 에 대해 $\tilde{\beta} = Ay$ 즉 $y$에 대해 linear인 임의의 estimator) 들 중 분산이 가장 작은 (즉 가장 효율적인) estimator임. 이를 Gauss-Markov theorem이라 함.</p>
  </li>
  <li>
    <p>Gauss-Markov theorem에서는 A5 (오차항의 정규분포 가정) 가 필요하지 않았음에 주의. 또한 biased되어 있거나 nonlinear인 estimator 중에서는 $\hat{\beta}_{OLS}$ 대비 분산이 더 작은 estimator가 존재할 수 있음 (이를 테면 $X$에 multicollinearity 문제가 있을 경우 이를 완화하기 위한 Ridge 회귀의 estimator는 biased estimator이지만 분산은 더 작음).</p>
  </li>
  <li>
    <p>A1, A2, A3Rsru, A4가 만족되면, 표본크기가 무한히 많아질 때 OLS estimator는 true $\beta$에 확률수렴함 (확률수렴이란 말은, $X$의 DGP를 고려 시 엄연히 확률변수인 $\hat{\beta}_{OLS}$가, $N$이 무한히 커질 경우 true $\beta$에 매우 가까운 값으로 나올 확률이 100%임을 의미). 
이는 $ \hat{\beta}_{OLS} - \beta = (X’X)^{-1}X’\epsilon = (X’X/N)^{-1} (X’\epsilon/N) $ 에 A3Rsru 조건 만족 시 $\text{plim} X’\epsilon/N=0$, 그리고 Slutsky theorem으로부터 $\text{plim}(X’X)^{-1}X’\epsilon$을  $\text{plim}(X’X/N)^{-1}=\Sigma_{xx}^{-1}$와 $\text{plim}X’\epsilon/N=0$의 곱으로 나타낼 수 있다는 사실로부터 유도됨.</p>
  </li>
  <li>
    <p>A1, A2, A3Rmi, A4가 만족되면, OLS estimator는 asymptotically normal, 즉 표본크기가 크다면 따로 A5 가정을 하지 않아도 근사적으로 정규분포를 따름. $X’\epsilon/N$을 $\epsilon_i$의 표본평균에 $X’$가 곱해진 것으로 해석하면, 중심극한정리에 의해 $\sqrt{N} (X’\epsilon/N - 0) \rightarrow N(0,\sigma^2 \Sigma_{xx})$ 이고 $\sqrt{N} ((X’X/N)^{-1}X’\epsilon/N - 0) = \sqrt{N}(\hat{\beta}_{OLS}-\beta) \rightarrow N(0,\sigma^2 \Sigma_{xx}^{-1})$ 이기 때문.</p>
  </li>
  <li>
    <p>실제로는 오차분산 $\sigma^2$를 정확히 알지 못하므로 표본 기반으로 추정함. A1, A2, A3Rmi, A4 조건이 만족될 경우, $s^2 = \sum_{i=1}^N \hat{\epsilon}^2/(N-k)$ 는 $\sigma^2$의 unbiased and consistent estimator임 ($k$는 intercept를 포함한 parameter의 수). 또한 A5 조건까지 만족될 경우, $(N-k) s^2/\sigma^2 \sim \chi^2(N-k)$ 임.</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 설명변수의 group 기준으로 분해해서 $y = X_1 \beta_1 + X_2 \beta_2 + \epsilon$ 으로 볼 때, $\beta_1$은 $X_2$가 통제된 상태에서 $X_1$만의 변화에 따른 $y$의 변화를 나타내야 함. 그러므로 $\hat{\beta}_1$을 구할 때는, $X_1$로부터 $X_2$의 정보를 projection matrix를 곱해 제거한 $(I - X_2(X_2’X_2)^{-1}X_2’)X_1 = M_2 X_1$ 을 기반으로 회귀해 구해야 함. 즉 $\hat{\beta}_1 = (X_1’M_2 X_1)^{-1} X_1’ M_2 y$ 임 (projection matrix가 idempotent, 즉 $M_2^2 = M_2$임을 이용함).</p>
  </li>
  <li>
    <p>선형회귀모델 $y = X \beta + \epsilon$ 을 위에서는 $X$의 column을 기준으로 분해했는데, 이와 달리 모델을 row 기준으로 분해할 경우 (이를테면 특정 행 기준으로 old와 new로 구분할 경우), $X’X \hat{\beta} = X’y$ 로부터 아래와 같이 됨. 
$ \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      X_{old} \ 
      X_{new}
  \end{bmatrix}<br />
  \hat{\beta} = 
  \begin{bmatrix}
      X_{old}’ &amp; X_{new}’
  \end{bmatrix}<br />
  \begin{bmatrix}
      y_{old} \ 
      y_{new}
  \end{bmatrix}<br />
  \, \Rightarrow \, [X_{old}’X_{old} + X_{new}’X_{new}] \hat{\beta} = X_{old}’ y_{old} + X_{new}’ y_{new} $
  이를 데이터의 갱신으로 보면 $X_{old}’X_{old}$ 및 $(X_{old}’X_{old})^{-1}$ 가 계산되어 있는 상황이므로, 새로운 데이터 행렬 전체에 대해 새로 계산할 필요 없이 아래 식으로부터 $\hat{\beta}$를 빠르게 계산할 수 있음.
  $ [X_{old}’X_{old} + X_{new}’X_{new}]^{-1} = (X_{old}’X_{old})^{-1} - (I+ X_{new} (X_{old}’X_{old})^{-1} X_{new}’ )^{-1} (X_{old}’X_{old})^{-1} X_{new}’X_{new} (X_{old}’X_{old})^{-1} $</p>
  </li>
</ul>

<h2 id="maximum-likelihood-estimation-in-linear-regression">Maximum likelihood estimation in linear regression</h2>

<ul>
  <li>
    <p>A1부터 A5까지의 가정들이 성립 시, $y_i = x_i’\beta + \epsilon_i$는 $x_i, \beta, \sigma^2$가 주어질 경우 오차항이 iid이며 정규분포를 따른다는 가정으로부터 $y_i | x_i, \beta, \sigma^2 \sim N(x_i’ \beta, \sigma^2)$ 임. 그러므로 $y$ 전체의 likelihood 및 log-likelihood는 아래와 같음.
$ L(y_i | x_i, \beta, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{(y_i - x_i’\beta)^2}{2 \sigma^2} \right)  = \frac{1}{\sqrt{(2\pi \sigma^2)^N}} \text{exp} \left( -\frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) \right)   $
$ \text{log} L(y_i | x_i, \beta, \sigma^2) = - \frac{N}{2} \text{log}(2 \pi) - \frac{N}{2} \text{log}(\sigma^2) - \frac{1}{2 \sigma^2} (y-X\beta)’(y-X\beta) $
여기서 log-likelihood에 $-(y-X\beta)’(y-X\beta)$ 가 포함되어 있으므로, $\sigma^2$가 주어질 때 log-likelihood를 최대화하는 추정량 $\hat{\beta}_{MLE}$는 $(y-X\beta)’(y-X\beta)$를 최소화하는 추정량, 즉 OLS 추정량과 같음.</p>
  </li>
  <li>
    <p>$\beta$가 주어질 때 log-likelihood를 최대화하는 $\sigma^2$의 추정량은 $\widehat{\sigma^2}_{MLE} = \hat{\epsilon}’ \hat{\epsilon}/N$, 즉 잔차제곱합을 표본 크기로 나눈 값이 됨. 이는 biased estimator임 (OLS에서의 unbiased estimator는 잔차제곱합을 $N-k$로 나눈 값이었음). 그러나 표본크기가 무한히 커지면 $N-k$와 $N$이 비슷해지므로, $\sigma^2$의 maximum likelihood 추정량은 consistent함.</p>
  </li>
  <li>
    <p>MLE에서 parameter를 $\theta$라 할 때 $\theta$의 분산은 log-likelihood의 Hessian의 평균에 마이너스를 붙인 후 역행렬을 취한 $(-\text{E}[H(\theta)])^{-1}$로 주어짐. 이 때 $(-\text{E}[H(\theta)])$ 를 information matrix $I(\theta)$라 부르기도 하며, $\text{Var}[\theta] = I(\theta)^{-1}$ 임. 
이에 따라 계산하면 $\text{Var}[\hat{\beta}_{MLE}] = \sigma^2(X’X)^{-1}$을 얻고 (OLS estimator의 공분산행렬과 같음), $\text{Var}[\widehat{\sigma^2}_{MLE}] = 2\sigma^4/N$을 얻음.</p>
  </li>
  <li>
    <p>MLE로 추정한 추정량은 consistent하며, asymptotic normal이고, Consistent Uniformly Asymptotically Normal (CUAN) 성질을 갖는 estimator들 중 가장 효율적인 추정량임 (MLE 추정량의 공분산행렬은 CUAN estimator들의 Cramer-Rao lower bound, 즉 unbiased estimator가 가지는 분산의 하한임). 그러므로 오차항의 분포에 확신이 있다면, MLE를 쓰는 것이 가장 좋음.</p>
  </li>
</ul>]]></content><author><name>Jeonghun Song</name></author><category term="Theory" /><summary type="html"><![CDATA[Ordinary least squares]]></summary></entry><entry><title type="html">A new example</title><link href="http://localhost:4000/2023/04/frompc.html" rel="alternate" type="text/html" title="A new example" /><published>2023-04-04T00:00:00+09:00</published><updated>2023-04-04T00:00:00+09:00</updated><id>http://localhost:4000/2023/04/frompc</id><content type="html" xml:base="http://localhost:4000/2023/04/frompc.html"><![CDATA[<p>A trivial file created in and pushed from my PC</p>

<p><img src="/assets/images/q1_2_qqplot.png" alt="q1_2_qqplot" /></p>

<p><img src="/assets/images/2023-04-04-frompc/EigenFace.png" alt="EigenFace" style="zoom:2000%;" /></p>

<p class="notice--primary">This is an eigenface</p>

<p><img src="/assets/images/2023-04-04-frompc/q2_4_scaledplot.png" alt="q2_4_scaledplot" class="align-center" /></p>

<div class="notice--danger">
<h4>This is an example of Fisher's linear discriminant analysis</h4>
<ul>
    <li> Two dimensional data </li>
    <li> Three classes </li>
</ul>
</div>

<p>$y = X \beta + \epsilon$ 일 때, OLS estimator는
$\hat{\beta} = (X’X)^{-1}X’y$ 로 주어짐</p>

<p><a href="https://product.kyobobook.co.kr/detail/S000001681559" class="btn btn--danger">추천교재</a></p>

<p><a href="/assets/files/linearalgebra_.pdf" class="btn btn--danger">정리본다운</a></p>

<!-- Courtesy of embedresponsively.com -->

<div class="responsive-video-container">
    <iframe src="https://www.youtube-nocookie.com/embed/ACzFIAOsfpM" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
  </div>]]></content><author><name>Jeonghun Song</name></author><category term="etc" /><summary type="html"><![CDATA[A trivial file created in and pushed from my PC]]></summary></entry></feed>