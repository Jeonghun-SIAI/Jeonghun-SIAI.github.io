<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BBZ8BXY4N7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BBZ8BXY4N7');
</script>
  <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 3-action control 도출 | 에너지엔데이터연구소</title>
<meta name="description" content="Vincent의 마이크로그리드 사례에서 Q-learning의 concept를 이용하기 위해, 실제로는 수전/송전이 continuous한 값임에도 불구하고, 1.1kW 수전/ 1.1kW 송전/ idle 의 3가지 action만을 고려하기로 했다. 각 action 별 인덱스는 0, 1, 2라 하자.">


  <meta name="author" content="Jeonghun Song">
  
  <meta property="article:author" content="Jeonghun Song">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="에너지엔데이터연구소">
<meta property="og:title" content="강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 3-action control 도출">
<meta property="og:url" content="https://song4energyndata.github.io/reinforcethree.html">


  <meta property="og:description" content="Vincent의 마이크로그리드 사례에서 Q-learning의 concept를 이용하기 위해, 실제로는 수전/송전이 continuous한 값임에도 불구하고, 1.1kW 수전/ 1.1kW 송전/ idle 의 3가지 action만을 고려하기로 했다. 각 action 별 인덱스는 0, 1, 2라 하자.">



  <meta property="og:image" content="https://song4energyndata.github.io/assets/images/reinforcethree/nn.png">





  <meta property="article:published_time" content="2023-06-11T00:00:00+09:00">





  

  


<link rel="canonical" href="https://song4energyndata.github.io/reinforcethree.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "송정훈",
      "url": "https://song4energyndata.github.io/"
    
  }
</script>






  <meta name="naver-site-verification" content="7dda6777a2acc897ba51668ac7c58c380307dd6c">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="에너지엔데이터연구소 Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<link rel="alternate" type="application/rss+xml" href="https://song4energyndata.github.io/feed.xml">
  <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<head>
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
</head>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- end custom head snippets -->
  <style> 
    ::-webkit-scrollbar{width: 12px;}
    ::-webkit-scrollbar-track {background-color:#4b4f52; border-radius: 8px;}
    ::-webkit-scrollbar-thumb {background-color:#5e6265; border-radius: 8px;}
    ::-webkit-scrollbar-thumb:hover {background: #f7f6f3;}
    ::-webkit-scrollbar-button:start:decrement,::-webkit-scrollbar-button:end:increment 
    {width:12px;height:16px;background:transparent;} 
  </style>
</head>

<body
  class="layout--single">
  <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

  

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          에너지엔데이터연구소
          <span class="site-subtitle">에너지 데이터의 올바른 활용, 에너지 전환에 대한 데이터 기반의 합리적 솔루션</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/aboutme/">About Me</a>
            </li><li class="masthead__menu-item">
              <a href="/search/">Search</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tag</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">카테고리 목록</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


  <div class="initial-content">
    





<div id="main" role="main">
  

  <div class="sidebar sticky">
    
  
  
  
  
    
    
      
    
    
  
  
   <!-- three lines added by Jeonghun  -->
    

<nav class="nav__list">
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">카테고리 목록</label>
  <ul class="nav__items" id="category_tag_menu">
      <!--전체 글 수-->
      
      <li>
        <!--span 태그로 카테고리들을 크게 분류 ex) C/C++/C#-->
        <span class="nav__sub-title">Categories</span>
            <!--ul 태그로 같은 카테고리들 모아둔 페이지들 나열-->
            <ul>
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/optimalsystem" class="">&#x2022 경제성분석 및 최적구성 도출 [10]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/market" class="">&#x2022 전력시장 모델링 [15]</a></li>
                    
                
                    
                
            </ul>
            <ul>
                <!--Cpp 카테고리 글들을 모아둔 페이지인 /categories/cpp 주소의 글로 링크 연결-->
                <!--category[1].size 로 해당 카테고리를 가진 글의 개수 표시--> 
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/energymanagement" class="">&#x2022 제어 (최적화/ 강화학습) [5]</a></li>
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/estimation" class="">&#x2022 예측 (회귀분석/ 머신러닝/ 딥러닝) [2]</a></li>
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/dataset" class="">&#x2022 데이터셋 (소개/ 전처리/ 활용안) [6]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                        <li><a href="/policy" class="">&#x2022 정책 (패널분석/ 시나리오분석) [1]</a></li>
                    
                
            </ul>
            <ul>
                
                    
                
                    
                        <li><a href="/mathstat" class="">&#x2022 수학/ 통계학 일반 [3]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                        <li><a href="/mypapers" class="">&#x2022 My papers [2]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
            <ul>
                
                    
                        <li><a href="/etc" class="">&#x2022 Etc. [2]</a></li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
      </li>
  </ul>
</nav>
  
  

  </div>




  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 3-action control 도출">
    <meta itemprop="description" content="Vincent의 마이크로그리드 사례에서 Q-learning의 concept를 이용하기 위해, 실제로는 수전/송전이 continuous한 값임에도 불구하고, 1.1kW 수전/ 1.1kW 송전/ idle 의 3가지 action만을 고려하기로 했다. 각 action 별 인덱스는 0, 1, 2라 하자.">
    <meta itemprop="datePublished" content="2023-06-11T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="https://song4energyndata.github.io/reinforcethree.html" class="u-url" itemprop="url">강화학습 기반 마이크로그리드 control - 3) Deep Q-Network를 통한 3-action control 도출
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-06-11T00:00:00+09:00">2023-06-11</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header> 
              <ul class="toc__menu"><li><a href="#심층신경망-구성-및-훈련-컨셉">심층신경망 구성 및 훈련 컨셉</a></li><li><a href="#dqn-코드">DQN 코드</a></li><li><a href="#dqn을-통한-discrete-control-결과">DQN을 통한 discrete control 결과</a></li><li><a href="#continuous-control을-한다면">Continuous control을 한다면?</a></li></ul>
 <!-- 우측 TOC -->
            </nav>
          </aside>
        
        <p>Vincent의 마이크로그리드 사례에서 Q-learning의 concept를 이용하기 위해, 실제로는 수전/송전이 continuous한 값임에도 불구하고, 1.1kW 수전/ 1.1kW 송전/ idle 의 3가지 action만을 고려하기로 했다. 각 action 별 인덱스는 0, 1, 2라 하자.</p>

<p><img src="/assets/images/reinforceone/system.png" alt="system" class="align-center" />
<em>Vincent의 연구에서 가정된 마이크로그리드.</em></p>

<p>이 때 심층신경망은 state를 입력받아 3개 action 각각의 Q-value의 추정치 $Q(s_{t},0), Q(s_{t},1), Q(s_{t},2)$ 를 출력으로 계산한다. 이 심층신경망은 매 훈련 주기마다 튜플 $(s_{t},a_{t},s_{t+1},r_{t+1})$ 의 batch를 받아서 업데이트된다.</p>

<p><br /></p>

<h2 id="심층신경망-구성-및-훈련-컨셉">심층신경망 구성 및 훈련 컨셉</h2>
<p>심층신경망 구성은 아래 그림과 같다.</p>

<p><img src="/assets/images/reinforcethree/nn.png" alt="nn" class="align-center" />
<em>Vincent의 연구에서 사용된 심층신경망 구성. (논문의 그림을 필자가 재구성하였음)</em></p>

<p>직전 24시간의 태양광 발전량은 첫 번째 Conv1D 층에, 직전 24시간의 부하는 두 번째 Conv1D 층에, 그리고 직전 1시간의 배터리 내 에너지양은 위 두 Conv1D층과 연결되는 Dense층에 바로 연결된다.</p>

<p>그리고 Dense층이 하나 더 있고, 이 층을 거쳐서 최종적으로 3개의 output node가 각 action별 Q-value를 출력한다. Conv1D 층을 쓰는 이유는, 태양광 발전량과 부하가 시간적인 정보를 갖고 있으므로 이를 반영하기 위함이다.</p>

<p>해당 심층신경망 훈련을 위한 training set은 특정 2년 (17,520시간) 동안의 시간별 태양광발전량과 부하 자료이다. 해당 모델이 직전 24시간의 정보를 state로 사용하므로 control은 $t=25,26,\cdots, 17520$ 에 대해 정해진다. Validation set은 별도의 특정 1년 (8,760시간) 동안의 시간별 태양광 발전량과 부하 자료이다.</p>

<p>Training set에 대해서는 $\epsilon$-greedy policy를, validation set에 대해서는 greedy policy를 적용해 action을 결정한다.</p>

<p>매 time step (데이터에서의 1시간) 별로 state와 action에 따른 마이크로그리드 내 에너지 흐름 및 수전/송전에 따른 손익 (reward) 와 이번 시점의 배터리 내 에너지 (다음 시점에서의 state임) 를 계산하고, 매 24시간 주기로 심층신경망 훈련을 gradient descent로 수행한다. 이를 training set의 17,520-24 시간에 대해 수행하면 1 epoch이다.</p>

<p>매 심층신경망 훈련 시 사용하는 데이터는 기본적으로 (state, action, reward, next state) 를 묶은 tuple로, 일종의 ‘경험’으로 볼 수 있다. 즉 시점 $t$에서 ($s_{t},a_{t},r_{t+1},s_{t+1}$) 를 추후 훈련에 쓸 수 있도록 buffer에 저장한다.</p>

<p>그리고 24시간 주기의 훈련에서는 직전 24시간의 tuple들이 아닌, buffer 내에서 랜덤하게 뽑은 32개 시간의 tuple들을 이용해 훈련시킨다. 이를 통해 훈련에 쓰이는 tuple들 간의 ‘시간적 상관성’을 감소시켜, 훈련의 효율성을 높일 수 있다.</p>

<p><br /></p>

<h2 id="dqn-코드">DQN 코드</h2>
<p>이제 훈련 concept는 대략 이해가 될 것이니, 코드를 보자.</p>

<p>이 코드는 필자가 직접 작성하였다. Vincent의 학위논문에서 해당 연구에 사용한 코드의 <a href="https://github.com/VinF/deer/tree/master/examples/MG_two_storages">Github 링크</a>를 제공하나, 각 모듈별로 코드 파일들을 지나치게 분리해 놓아 사용이 여의치 않았다. 이에 필자가 ‘모든 기능들을 한 파일에 넣은 공부용’ 코드를 따로 만들었다. (<a href="https://github.com/song4energyndata/Codes/tree/main/reinforcementlearning/microgrid_Vincent">GitHub Repo 링크</a>)</p>

<p>(코드 작성 시 <a href="https://product.kyobobook.co.kr/detail/S000001810262">핸즈온 머신러닝</a>의 강화학습 chapter의 Cartpole 예제에 대한 Deep Q-Network 코드를 참고하였다.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -*- coding: utf-8 -*-
</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="c1">### hyperparameters
</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">)</span> <span class="c1"># learning rate 너무 높으면 발산할 수 있음에 주의
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.98</span>
<span class="n">period_step_fortrain</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">rewardscalefactor</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">6</span>
<span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">20000</span>



<span class="c1">### microgrid system data
</span>
<span class="c1"># 시간별 데이터 출처: https://github.com/VinF/deer/tree/master/examples/MG_two_storages/data
</span><span class="n">PV_prod_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'BelgiumPV_prod_train.npy'</span><span class="p">)</span> <span class="c1"># [0,1] 구간 내로 scaled된 데이터임, unscaling은 play_one_step 함수 내에서 이루어짐
</span><span class="n">PV_prod_test</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'BelgiumPV_prod_test.npy'</span><span class="p">)</span> 

<span class="n">load_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'example_nondeterminist_cons_train.npy'</span><span class="p">)</span> <span class="c1"># [0,1] 구간 내로 scaled된 데이터임, unscaling은 play_one_step 함수 내에서 이루어짐
</span><span class="n">load_test</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'example_nondeterminist_cons_test.npy'</span><span class="p">)</span>

<span class="n">prate_h2</span> <span class="o">=</span> <span class="mf">1.1</span> <span class="c1"># 수전/송전 상한
</span><span class="n">eff_h2</span> <span class="o">=</span> <span class="mf">0.65</span> <span class="c1"># 수소-전기 변환효율
</span>
<span class="n">capa_batt</span> <span class="o">=</span> <span class="mi">15</span> <span class="c1"># 배터리 용량 (겉보기용량 말고 SOC 상하한 고려한 실용량이라 가정)
</span><span class="n">eff_batt</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c1"># 배터리 충방전 효율
</span><span class="n">initialenergy_batt</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># epoch의 시작에서 배터리 내 에너지량 (최대저장가능량 대비 상대비율, Qval NN에는 이 상대비율이 입력되며, play_one_step 함수 내에서의 에너지시스템 밸런스 수식에서는 실제 에너지값으로 변환됨
</span>
<span class="n">price_h2</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">cost_loss</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">load_peak</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pv_peak</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">inputlen_load</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">inputlen_pv</span> <span class="o">=</span> <span class="mi">24</span>



<span class="c1">### Neural net configuration
</span>
<span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">input_load</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 과거 24시간의 부하, shape의 두번째 숫자는 채널 수 (컬러사진의 RGB 등), 채널 수를 정의해야 Conv1D가 작동함
</span><span class="n">input_pv</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 과거 24시간의 태양광발전량
</span><span class="n">hidden_conv_load</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">input_load</span><span class="p">)</span>
<span class="n">hidden_conv_pv</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">input_pv</span><span class="p">)</span>
<span class="n">concat_conv</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">hidden_conv_load</span><span class="p">,</span><span class="n">hidden_conv_pv</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">hidden_conv_concat</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">concat_conv</span><span class="p">)</span>
<span class="n">flatten_conv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">()(</span><span class="n">hidden_conv_concat</span><span class="p">)</span> <span class="c1"># Dense층 직전에 Conv층을 Flatten해야 함
</span><span class="n">input_others</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># 직전 시간의 배터리 내 에너지량
</span><span class="n">concat_all</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">flatten_conv</span><span class="p">,</span><span class="n">input_others</span><span class="p">])</span> 
<span class="n">hidden_dense_1</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">concat_all</span><span class="p">)</span>
<span class="n">hidden_dense_2</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">hidden_dense_1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)(</span><span class="n">hidden_dense_2</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>



<span class="k">def</span> <span class="nf">e_greedy_policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="c1"># epsilon-greedy policy
</span>    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span> <span class="c1"># epsilon의 확률로 exploration
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># action을 랜덤하게 선택
</span>    <span class="k">else</span><span class="p">:</span> <span class="c1"># exploitation
</span>        <span class="n">input_load</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">inputlen_load</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Conv1D의 input이므로 채널 수 1 명시
</span>        <span class="n">input_pv</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="n">inputlen_load</span><span class="p">:(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">input_others</span> <span class="o">=</span><span class="n">state</span><span class="p">[(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Dense층의 input이므로 채널 수는 필요 없음
</span>        <span class="n">Q_values</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">))</span> <span class="c1"># 각 action 별 Q-value 도출
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 가장 큰 Q-value에 대응하는 action 선택      
</span>        <span class="c1"># 주의: for loop 안에서 DNN에 input을 입력해 output 계산 시 model()로 해야지, model.predict()로 하면 안 됨! 메모리 누수가 발생함 (model.predict는 대량의 input data를 'model.predict를 한 번만 호출해서' 처리하는 데 특화됨, https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict 참고)
</span>


<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span> <span class="c1"># replay buffer 정의
</span>
<span class="k">def</span> <span class="nf">sample_experiences</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span> <span class="c1"># batch_size만큼의 경험들의 state들, action들, reward들, nextstate들의 리스트들을 반환
</span>    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span> <span class="c1"># replay buffer 내 경험들 중 랜덤하게 batch_size만큼의 경험들을 지정 (인덱스 불러옴)
</span>    <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">replay_buffer</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span> <span class="c1"># 위에서 불러온 인덱스를 이용해 batch_size 만큼의 경험들을 batch 리스트에 담음
</span>    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">experience</span><span class="p">[</span><span class="n">field_index</span><span class="p">]</span> <span class="k">for</span> <span class="n">experience</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span> <span class="c1"># 하나의 array 안에 batch 내 각 경험별 값들이 들어감 
</span>        <span class="k">for</span> <span class="n">field_index</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span> <span class="c1"># 총 4개의 array를 반환, 각각은 (batch 내 sample경험들의) state들, action들, reward들, nextstate들의 리스트임
</span>    <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span>



<span class="k">def</span> <span class="nf">play_one_step</span><span class="p">(</span><span class="n">profile_load</span><span class="p">,</span><span class="n">profile_pv</span><span class="p">,</span><span class="n">hour</span><span class="p">,</span><span class="n">energy_batt</span><span class="p">,</span><span class="n">epsilon</span><span class="p">,</span><span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span> <span class="c1"># 마이크로그리드 시스템의 시간별 operation 모델링
</span>        
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">(</span><span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="n">inputlen_load</span><span class="p">:</span><span class="n">hour</span><span class="p">],</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="n">inputlen_pv</span><span class="p">:</span><span class="n">hour</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">energy_batt</span><span class="p">]))</span> <span class="p">)</span>
    
    <span class="n">action</span> <span class="o">=</span> <span class="nf">e_greedy_policy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">epsilon</span><span class="p">)</span> <span class="c1"># epsilon-greedy policy에 따라 action 도출
</span>    <span class="n">p_h2_send</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p_h2_receive</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># 수소로 생산된 전기 수전
</span>        <span class="n">p_h2_receive</span> <span class="o">=</span> <span class="n">prate_h2</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># 수소 생산용 전기 송전
</span>        <span class="n">p_h2_send</span> <span class="o">=</span> <span class="n">prate_h2</span>
       

    <span class="n">p_load</span> <span class="o">=</span> <span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="p">]</span><span class="o">*</span><span class="n">load_peak</span> <span class="c1"># 현재 시간의 부하 (action 결정 기준이 아님!), 입력자료가 [0,1]로 scaled된 걸 unscaling
</span>    <span class="n">p_pv</span> <span class="o">=</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="p">]</span><span class="o">*</span><span class="n">pv_peak</span> <span class="c1"># 현재 시간의 발전량 (action 결정 기준이 아님!), 입력자료가 [0,1]로 scaled된 걸 unscaling  
</span>    <span class="n">energy_batt</span> <span class="o">=</span> <span class="n">energy_batt</span><span class="o">*</span><span class="n">capa_batt</span> <span class="c1"># 입력자료가 [0,1]로 scaled된 걸 unscaling
</span>    
    <span class="c1">#p_curtail = 0
</span>    <span class="n">p_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">p_net_beforebatt</span> <span class="o">=</span> <span class="n">p_pv</span> <span class="o">-</span> <span class="n">p_load</span> <span class="o">+</span> <span class="n">p_h2_receive</span> <span class="o">-</span> <span class="n">p_h2_send</span> <span class="c1"># 태양광 생산, 부하 충족, H2 보내거나 받은 후 수용가 입장에서 전기에너지가 남으면 양수, 부족하면 음수
</span>    
    <span class="k">if</span> <span class="n">p_net_beforebatt</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># 전기에너지가 남으므로 배터리에 저장하고, 만약 배터리도 꽉 찬다면 curtail함
</span>        <span class="k">if</span> <span class="n">capa_batt</span> <span class="o">&gt;=</span> <span class="n">energy_batt</span> <span class="o">+</span> <span class="n">p_net_beforebatt</span><span class="o">*</span><span class="n">eff_batt</span><span class="p">:</span> <span class="c1"># 배터리에 잔여 충전 가능한 에너지가 위에서 남은 에너지(에서 변환손실 제한 에너지) 이상일 경우
</span>            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">energy_batt</span> <span class="o">+</span> <span class="n">p_net_beforebatt</span><span class="o">*</span><span class="n">eff_batt</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># 남은 에너지를 배터리에 다 충전하면 꽉 차고도 남아서, curtail해야 함
</span>            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">capa_batt</span>
            <span class="c1">#p_curtail = (energy_batt + p_net_beforebatt*eff_batt - capa_batt)/eff_batt # 괄호 안은 배터리 내부 기준이며, 수용가 모선 기준 양을 구하려면 eff_batt로 나눠줘야 함
</span>    <span class="k">else</span><span class="p">:</span> <span class="c1"># 전기에너지가 부족하므로 배터리 에너지를 써야 함, 배터리 에너지로도 부족하면 loss임
</span>        <span class="k">if</span> <span class="n">energy_batt</span><span class="o">*</span><span class="n">eff_batt</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">p_net_beforebatt</span><span class="p">:</span> <span class="c1"># 배터리 에너지로 충당 가능한 경우
</span>            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">energy_batt</span> <span class="o">+</span> <span class="n">p_net_beforebatt</span><span class="o">/</span><span class="n">eff_batt</span> <span class="c1"># p_net_beforebatt가 음수이므로 마이너스값이 부족한 에너지'량'이고 그걸 '빼'므로 결과적으로 플러스
</span>        <span class="k">else</span><span class="p">:</span> <span class="c1"># 부족해 loss 발생
</span>            <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">p_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">p_net_beforebatt</span> <span class="o">-</span> <span class="n">energy_batt</span><span class="o">*</span><span class="n">eff_batt</span>           
    
    <span class="n">reward</span> <span class="o">=</span> <span class="n">price_h2</span><span class="o">*</span><span class="n">p_h2_send</span><span class="o">*</span><span class="n">eff_h2</span> <span class="o">-</span> <span class="n">price_h2</span><span class="o">*</span><span class="n">p_h2_receive</span><span class="o">/</span><span class="n">eff_h2</span> <span class="o">-</span> <span class="n">cost_loss</span><span class="o">*</span><span class="n">p_loss</span>
    <span class="n">energy_batt_after</span> <span class="o">=</span> <span class="n">energy_batt_after</span><span class="o">/</span><span class="n">capa_batt</span> <span class="c1"># 배터리 내 저장량을 [0,1] 범위로 scaling함
</span>    
    <span class="k">if</span> <span class="n">training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span> <span class="p">(</span><span class="n">profile_load</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="p">(</span><span class="n">inputlen_load</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span><span class="n">hour</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">profile_pv</span><span class="p">[</span><span class="n">hour</span><span class="o">-</span><span class="p">(</span><span class="n">inputlen_pv</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span><span class="n">hour</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">energy_batt_after</span><span class="p">]))</span> <span class="p">)</span>         
        <span class="n">replay_buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="o">*</span><span class="n">rewardscalefactor</span><span class="p">,</span> <span class="n">next_state</span><span class="p">))</span> <span class="c1"># replay buffer에는 scaled reward를 넣으며, tuple을 append함에 주의
</span>    <span class="k">return</span> <span class="n">energy_batt_after</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">action</span> <span class="c1"># scaled 배터리 내 저장량, unscaled reward, action index 반환
</span>


<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span> <span class="c1"># 심층신경망에 대해 Gradient descent 기반 가중치 업데이트 수행
</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="nf">sample_experiences</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        
    <span class="n">input_load</span> <span class="o">=</span> <span class="n">states</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">inputlen_load</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> 
    <span class="n">input_pv</span> <span class="o">=</span> <span class="n">states</span><span class="p">[:,</span><span class="n">inputlen_load</span><span class="p">:(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_others</span> <span class="o">=</span> <span class="n">states</span><span class="p">[:,(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">input_load_next</span> <span class="o">=</span> <span class="n">next_states</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">inputlen_load</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_load</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_pv_next</span> <span class="o">=</span> <span class="n">next_states</span><span class="p">[:,</span><span class="n">inputlen_load</span><span class="p">:(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">inputlen_pv</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">input_others_next</span> <span class="o">=</span> <span class="n">next_states</span><span class="p">[:,(</span><span class="n">inputlen_load</span><span class="o">+</span><span class="n">inputlen_pv</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
      
    <span class="n">next_Q_values</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">input_load_next</span><span class="p">,</span><span class="n">input_pv_next</span><span class="p">,</span><span class="n">input_others_next</span><span class="p">))</span> <span class="c1"># Q-learning을 위해, 'next'state에서의 각 action 별 Q-value 추정치들 반환
</span>    <span class="n">max_next_Q_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">next_Q_values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 각 경험별로 nextstate에 대한 Q-value가 더 높은 행동의 Q-value 사용
</span>    <span class="n">target_Q_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards</span> <span class="o">+</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">max_next_Q_values</span><span class="p">)</span> <span class="c1"># Q-value를 reward와 nextstate에 대한 Q-value(discounted)의 합으로 표현
</span>    <span class="n">target_Q_values</span> <span class="o">=</span> <span class="n">target_Q_values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 뒤의 Q_values와 차원 맞춰줌
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">n_outputs</span><span class="p">)</span> <span class="c1"># 각 경험 별 action을 one-hot encoding 형태로 만들어줌 (각 행이 경험)
</span>    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span> <span class="c1"># 자동 미분
</span>        <span class="n">all_Q_values</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">input_load</span><span class="p">,</span><span class="n">input_pv</span><span class="p">,</span><span class="n">input_others</span><span class="p">))</span> 
        <span class="n">Q_values</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">all_Q_values</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="c1"># mask를 곱해서, 각 경험별로 그 state에서 취하지 않은 action에 대해서는 Q-value에 0이 곱해지도록 함
</span>                                 <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># 각 행 별 합을 구함, 위의 masking 덕분에 그 state에서 취한 action에 대한 Q-value가 됨, keepdims를 True로 설정해 2차원 행렬 형태 유지
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">target_Q_values</span><span class="p">,</span><span class="n">Q_values</span><span class="p">))</span> <span class="c1"># 평균제곱오차 계산
</span>    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span> <span class="c1"># loss 함수를 model의 trainable variables 전체에 대해 미분
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span> <span class="c1"># Adam optimizer로 parameter update 수행, zip으로 gradient와 parameter pair를 맞춰줌
</span>


<span class="n">profits_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">elapsedtime_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">count_step_fortrain</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">max_return_test</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span> 
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># 맨 첫 번째 epoch에서는 훈련을 시작하지 않고 buffer를 채움, 두 번째 epoch부터는 buffer에 sample들이 채워졌으므로 훈련함
</span>        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    
    <span class="c1">### Train for each epoch  
</span>    <span class="n">hour</span> <span class="o">=</span> <span class="mi">24</span> <span class="c1"># 시작시점
</span>    <span class="n">energy_batt</span> <span class="o">=</span> <span class="n">initialenergy_batt</span>  <span class="c1"># 배터리 내 에너지의 초기값
</span>    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">load_train</span><span class="p">)</span><span class="o">-</span><span class="mi">25</span><span class="p">):</span> <span class="c1"># 데이터 내의 마지막 시점이 nextstate에만 포함될 때까지 (대신 termination state는 따로 없음)
</span>        <span class="n">count_step_fortrain</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epoch</span> <span class="o">/</span> <span class="mi">50</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span> <span class="c1"># epsilon을 1에서 시작해 조금씩 선형적으로 줄임, 이 경우 초반에는 거의 다 exploration이므로 한 epoch가 매우 빨리 계산되나, 중반을 넘어가면 거의 exploitation이며 이 때 매 step마다 DNN을 call하므로 느려짐
</span>        <span class="n">energy_batt</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="nf">play_one_step</span><span class="p">(</span><span class="n">load_train</span><span class="p">,</span><span class="n">PV_prod_train</span><span class="p">,</span><span class="n">hour</span><span class="p">,</span><span class="n">energy_batt</span><span class="p">,</span><span class="n">epsilon</span><span class="p">,</span><span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># energy_batt가 반복 갱신됨
</span>        <span class="n">hour</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># 다음 시간으로  
</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">count_step_fortrain</span> <span class="o">&gt;</span> <span class="n">period_step_fortrain</span><span class="p">:</span> <span class="c1"># 훈련 주기, 매 시간마다 train하면 epoch당 시간이 너무 길어지고 overfitting 우려도 있어, 매 24시간마다 훈련함, 첫 epoch에서는 replay buffer를 채우기만 하고, 나머지 99 epoch 동안 심층신경망 가중치를 업데이트함
</span>                <span class="nf">training_step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="c1"># DNN 훈련을 위한 Gradient Descent 수행
</span>                <span class="n">count_step_fortrain</span> <span class="o">=</span> <span class="mi">0</span>


    <span class="c1">### Validation for each epoch  
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">testcase_actions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">testcase_battenergy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">epoch_return_test</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># return (각 시점별 수익의 총 합) 초기화
</span>        
        <span class="n">hour</span> <span class="o">=</span> <span class="mi">24</span> <span class="c1"># 시작시점   
</span>        <span class="n">energy_batt</span> <span class="o">=</span> <span class="n">initialenergy_batt</span> <span class="c1"># 배터리 내 에너지의 초기값
</span>        
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">load_test</span><span class="p">)</span><span class="o">-</span><span class="mi">24</span><span class="p">):</span> <span class="c1"># 데이터 내의 마지막 시점이 nextstate에만 포함될 때까지 (대신 termination state는 따로 없음)
</span>            <span class="n">energy_batt</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="nf">play_one_step</span><span class="p">(</span><span class="n">load_test</span><span class="p">,</span><span class="n">PV_prod_test</span><span class="p">,</span><span class="n">hour</span><span class="p">,</span><span class="n">energy_batt</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># energy_batt가 반복 갱신됨
</span>            <span class="n">testcase_actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">testcase_battenergy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">energy_batt</span><span class="p">)</span>
            <span class="n">epoch_return_test</span> <span class="o">+=</span> <span class="n">reward</span> <span class="c1"># 누적보상 계산
</span>            <span class="n">hour</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># 다음 시간으로    
</span>        
        <span class="n">profits_test</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">epoch_return_test</span><span class="p">)</span> <span class="c1"># 각 epoch별 총 수익 로그 저장
</span>        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_profit_test_dqn.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">profits_test</span><span class="p">:</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                
        <span class="k">if</span> <span class="n">max_return_test</span> <span class="o">&lt;</span> <span class="n">epoch_return_test</span><span class="p">:</span> <span class="c1"># Validation set에서의 총 수익의 최대값 갱신시마다 저장 (unlearn하게 되더라도 중간에 best performance였던 모델을 남김)
</span>            <span class="n">max_return_test</span> <span class="o">=</span> <span class="n">epoch_return_test</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">save_weights</span><span class="p">(</span><span class="s">'trainedmodel_dqn.h5'</span><span class="p">)</span> <span class="c1"># 모델 가중치 저장
</span>                    
            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_actions_test_dqn.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="c1"># Validation case에서의 시간별 action 로그 저장
</span>                    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">testcase_actions</span><span class="p">:</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
            
            <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_battenergy_test_dqn.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="c1"># Validation case에서의 시간별 배터리 내 저장된 에너지 로그 저장
</span>                    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">testcase_battenergy</span><span class="p">:</span>
                        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                        
        <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>   
        <span class="n">elapsedtime_test</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">"Validation: profit of epoch {} is {}, maximum profit is {}"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">epoch_return_test</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="nf">round</span><span class="p">(</span><span class="n">max_return_test</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'one epoch 수행에 {}초 걸렸습니다'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">round</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">'trajectory_time_test_dqn.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="c1"># 각 epoch별 소요시간 로그 저장
</span>            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">elapsedtime_test</span><span class="p">:</span>
                <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">line</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Epoch당 계산 시간이 긴데, 이유는 매 시간 별 action을 결정할 때마다 심층신경망에 state를 입력해 Q-value들을 계산하는 과정을, training case에 대해서는 이를 8760x2-24번, validation case에 대해서는 8760-24번을 수행해야 하기 때문이다.</p>

<p><br /></p>

<h2 id="dqn을-통한-discrete-control-결과">DQN을 통한 discrete control 결과</h2>
<p>훈련을 수행하면, validation case 기준으로 불과 몇 epoch만에 누적 비용이 200유로 이하로 줄어든다. 그리고 99 epoch 훈련 동안 validation case에 대해 가장 좋은 결과를 보인 모델의 경우 누적 비용이 27유로이다.</p>

<p><img src="/assets/images/reinforcethree/trainingrecord.png" alt="trainingrecord" class="align-center" width="80%" />
<em>훈련 epoch 별 validation case의 누적 ‘수익’.</em></p>

<p>누적비용 27유로는, 미래를 안다고 가정하고 선형계획법 (Linear Programming, LP) 으로 도출한 control 시의 누적비용 -50유로에 근접한 값이다.</p>

<p>그러므로, 미래를 모르면서 과거 24시간 정보만 갖고 도출한 ‘discrete’ control 것 치고는 훌륭해 보인다. (Random control 시의 누적비용 2500~2700유로와 비교하면 큰 개선이다.)</p>

<p>시간별 action (계통으로부터의 수전) 이 LP와 DQN에서 각각 어떻게 결정되었는지를 아래 그림과 같이 보자.</p>

<p><img src="/assets/images/reinforcethree/result_dqn.png" alt="result_dqn" class="align-center" />
<em>DQN으로 도출한 수전/송전 control. LP control과 어느 정도 비슷함.</em></p>

<p>LP와 DQN으로 도출한 두 control들이 나름 비슷하다. ‘대체로’ LP control에서 수전하는 시간대에 DQN control에서도 수전하고, LP control에서 송전하는 시간대에 DQN control에서도 송전하며, LP control에서 idle인 시간대에 DQN control에서도 idle이다.</p>

<p>특히 LP control은 [-1.1,1.1] 범위 내의 실수로 주어지는 ‘continuous’ control임에도 불구하고, DQN의 action들에 대응하는 최대치 송전/최대치 수전/ idle인 경우가 많다는 점이 특기할 만하다. 이 덕분에, 3-action DQN으로 우수한 economic control을 얻을 수 있는 것으로 보인다.</p>

<p><br /></p>

<h2 id="continuous-control을-한다면">Continuous control을 한다면?</h2>
<p>그렇지만, LP control에서와 DQN control 간에 최대치 송전/ 최대치 수전/ idle 지속시간의 차이가 있고, LP control에서 이 3개 값이 아닌 다른 값인 경우도 (즉 최대치가 아닌 양으로 송전/수전하는 경우도) 분명히 많이 존재한다.</p>

<p>그렇다면 [-1.1,1.1] 범위 내의 실수로 action을 도출하는, continuous control 심층강화학습 기법을 쓰면 어떨까? 최대치가 아닌 송전/ 수전의 가능성까지 반영한 controller를 훈련함으로써, validation case에 대해 더 낮은 누적비용을 달성하는 control 도출이 가능할까?</p>

<p>다음 포스팅에서는, continuous control을 위한 심층강화학습 중 가장 기본적이면서 중요한 방법인 Deep Deterministic Policy Gradient (DDPG) 를 적용하는 방법 및 결과를 설명한다.</p>

<div class="notice--info">

강화학습 기반 마이크로그리드 control<br /><br />

1) <a href="/reinforceone.html">미래를 모를 때의 '경제적' control을 위한 강화학습</a><br />
2) <a href="/reinforcetwo.html">강화학습의 기본, Q-learning 리뷰</a><br />
3) <b>Deep Q-Network를 통한 3-action control 도출</b><br />
4) <a href="/reinforcefour.html">DDPG를 이용한 'continuous' control 도출</a><br />
5) <a href="/reinforcefive.html">TD3/ SAC 등 '진보된' continuous control을 쓴다면?</a>

</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#python" class="page__taxonomy-item p-category" rel="tag">Python</a><span class="sep">, </span>
    
      <a href="/tags/#%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5" class="page__taxonomy-item p-category" rel="tag">강화학습</a><span class="sep">, </span>
    
      <a href="/tags/#%EB%85%B9%EC%83%89%EC%84%AC" class="page__taxonomy-item p-category" rel="tag">녹색섬</a>
    
    </span>
  </p>



<!-- 
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#energymanagement" class="page__taxonomy-item p-category" rel="tag">energymanagement</a>
    
    </span>
  </p>

 -->
        <!-- 

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2023-06-11T00:00:00+09:00">2023-06-11</time></p>
 -->
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <!-- <a href="https://twitter.com/intent/tweet?text=%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5+%EA%B8%B0%EB%B0%98+%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EA%B7%B8%EB%A6%AC%EB%93%9C+control+-+3%29+Deep+Q-Network%EB%A5%BC+%ED%86%B5%ED%95%9C+3-action+control+%EB%8F%84%EC%B6%9C%20http%3A%2F%2Flocalhost%3A4000%2Freinforcethree.html" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> -->

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Freinforcethree.html" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Freinforcethree.html" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      <p></p>
        <!-- <h4 class="page__meta-title"><span>energymanagement</span> <span>카테고리 내 다른 글 보러가기</span></h4> -->
        


  

  

  
  	
  	
  	
  	
  	

<nav class="pagination_prev_next"> <!-- 식빵맘 코드에서 조금 수정함 -->

  
    
      <a href="/reinforcefour.html" class="pagination_prev_next--pager"><span class="prev_next">다음 글  &nbsp  </span>강화학습 기반 마이크로그리드 control - 4) DDPG를 이용한 'continuous' control 도출</a>
    
    
      <a href="/reinforcetwo.html" class="pagination_prev_next--pager"><span class="prev_next">이전 글  &nbsp</span>강화학습 기반 마이크로그리드 control - 2) 강화학습의 기본, Q-learning 리뷰</a>
        
  

</nav>
      <p></p>
    </div>

    
  </article>

  
  <!-- 
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/koreanmarketrules.html" rel="permalink">국내 전력시장 주요 운영규칙 핵심사항 정리
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-10-22T00:00:00+09:00">2023-10-22</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">최근 에너지 플랫폼 업계 현직자들과 대화를 하며, 국내 전력시장 운영규칙을 상세히 알 필요성을 느꼈다. 본격적으로 에너지 플랫폼 업계에 종사한다면 전력시장 입찰을 전제로 한 에너지 설비들의 데이터 및 비즈니스 모델을 주로 분석하게 될 것이라 생각했기 때문이다. (그간 필자는 BTM...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <a href="/evestimate.html" rel="permalink"><img src="/assets/images/evestimates/teaser.png" alt=""></a>
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/evestimate.html" rel="permalink">논문 소개: 전기차 충전 이력 데이터 기반의, 계통 내 시간별 전기차 충전 부하 시뮬레이션
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-10-21T00:00:00+09:00">2023-10-21</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">전기차 및 충전 인프라 규모가 증가할수록 전력계통에 걸리는 부하가 증가한다. 그러므로 향후 발전/ 송전/ 배전 (특히 배전단) 설비 및 스케줄링 계획 수립을 위해, 전기차 충전으로 인한 시간별 부하 증가를 정확히 예측할 필요가 있다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <a href="/marketadvanced.html" rel="permalink"><img src="/assets/images/jalal/advanced.png" alt=""></a>
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/marketadvanced.html" rel="permalink">재생발전 비중이 높은 전력시장에 대한 심화 주제들
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-10-07T00:00:00+09:00">2023-10-07</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">필자가 정리했던 Jalal Kazempour 교수의 전력시장 강의에서 reference로써 추천한 책으로, Integrating Renewables in Electricity Markets: Operational Problems 이 있다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <a href="/energyeffpanel.html" rel="permalink"><img src="/assets/images/energyeffpanel/teaser.png" alt=""></a>
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/energyeffpanel.html" rel="permalink">에너지효율 R&amp;D 투자효과 분석 (패널 데이터 기반) 연구 복원
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2023-09-24T00:00:00+09:00">2023-09-24</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">에너지정책의 효과를 ‘정량적으로’ 분석하는 방법으로, 회귀분석의 일종인 ‘패널 분석’ (Panel data analysis) 이 있다. 이를테면 에너지경제연구원에서는 에너지효율 R&amp;D 투자가 에너지소비 감소에 기여했는지 여부를 검정하기 위해, “에너지효율향상 R&amp;D ...</p>
  </article>
</div>

        
      </div>
    </div> -->
  
  <!--  -->
</div>

  </div>

  

  <aside class="sidebar__top">
    <a href="#site-nav"> <i class="fas fa-angle-double-up fa-2x"></i></a>
    </aside>

  <div id="footer" class="page__footer">
    <footer>
      <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
      <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Contact:</strong></li>
    

    
      
        
          <li><a href="mailto:song4energy@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://github.com/song4energyndata" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 송정훈. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

    </footer>
  </div>

  
  <script src="/assets/js/main.min.js"></script>










</body>

</html>